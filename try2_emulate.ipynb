{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "def normalize3(a, min_a=None, max_a=None):\n",
    "    if min_a is None: min_a, max_a = np.min(a, axis=0), np.max(a, axis=0)\n",
    "    return ((a - min_a) / (max_a - min_a + 0.0001)), min_a, max_a\n",
    "\n",
    "def denormalize3(a_norm, min_a, max_a):\n",
    "    return a_norm * (max_a - min_a + 0.0001) + min_a\n",
    "\n",
    "def trunc(values, decs=0):\n",
    "    return np.trunc(values*10**decs)/(10**decs)\n",
    "\n",
    "def init_db(feature_set, db_name=\"masters_data.db\", table_name=\"severity_trending\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table if it does not exist\n",
    "    columns = \", \".join([feature_name.replace(\" \", \"_\") for feature_name in feature_set])\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            timestamp TEXT,\n",
    "            {columns}\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def init_db_timeconst(feature_set, db_name=\"masters_data.db\", table_name=\"severity_trending\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table if it does not exist\n",
    "    columns = \", \".join([feature_name.replace(\" \", \"_\") for feature_name in feature_set])\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            timestamp TEXT,\n",
    "            {columns}\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def trend_savedb(data, feature_set, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    timestamp = datetime.now().replace(second=0, microsecond=0).isoformat()\n",
    "    cursor.execute(f\"\"\"\n",
    "        INSERT INTO {table_name} (timestamp, {', '.join([feature_name.replace(\" \", \"_\") for feature_name in feature_set])})\n",
    "        VALUES (?, {', '.join(['?' for _ in range(len(feature_set))])})\n",
    "    \"\"\", (timestamp, *data))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def timeseries_savedb(df_timestamp, data, feature_set, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    # Generate timestamp\n",
    "    timestamp = df_timestamp.isoformat()\n",
    "    \n",
    "    # Build column names for features, replacing spaces with underscores\n",
    "    feature_columns = ', '.join([feature_name.replace(\" \", \"_\") for feature_name in feature_set])\n",
    "    placeholders = ', '.join(['?' for _ in range(len(feature_set))])\n",
    "    \n",
    "    # Upsert using INSERT OR REPLACE\n",
    "    # Note: Your table must have a UNIQUE constraint on the timestamp column.\n",
    "    sql = f\"\"\"\n",
    "        INSERT OR REPLACE INTO {table_name} (timestamp, {feature_columns})\n",
    "        VALUES (?, {placeholders})\n",
    "    \"\"\"\n",
    "    cursor.execute(sql, (timestamp, *data))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def batch_timeseries_savedb(df_timestamps, data, feature_set, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Convert timestamps to ISO format\n",
    "    timestamps = [pd.to_datetime(ts).isoformat() for ts in df_timestamps]\n",
    "    \n",
    "    # Build column names for features, replacing spaces with underscores\n",
    "    feature_columns = ', '.join([feature_name.replace(\" \", \"_\") for feature_name in feature_set])\n",
    "    placeholders = ', '.join(['?' for _ in range(len(feature_set)+1)])  # 30 features + 1 timestamp\n",
    "    \n",
    "    # Prepare batch data\n",
    "    batch_data = [(timestamps[i], *data[i]) for i in range(data.shape[0])]\n",
    "    \n",
    "    # Upsert using INSERT OR REPLACE (Ensure UNIQUE constraint on timestamp in your DB schema)\n",
    "    sql = f\"\"\"\n",
    "        INSERT OR REPLACE INTO {table_name} (timestamp, {feature_columns})\n",
    "        VALUES ({placeholders})\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.executemany(sql, batch_data)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_between_dates(start_date, end_date, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT * FROM {table_name} WHERE timestamp BETWEEN ? AND ?\n",
    "    \"\"\", (start_date, end_date))\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not rows:\n",
    "        return np.array([])\n",
    "    \n",
    "    return np.array(rows)\n",
    "\n",
    "def convert_timestamp(timestamp_str):\n",
    "    dt = datetime.fromisoformat(timestamp_str)\n",
    "    return pd.Timestamp(dt.strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = ['Active Power', 'Reactive Power', 'Governor speed actual', 'UGB X displacement', 'UGB Y displacement',\n",
    "    'LGB X displacement', 'LGB Y displacement', 'TGB X displacement',\n",
    "    'TGB Y displacement', 'Stator winding temperature 13',\n",
    "    'Stator winding temperature 14', 'Stator winding temperature 15',\n",
    "    'Surface Air Cooler Air Outlet Temperature',\n",
    "    'Surface Air Cooler Water Inlet Temperature',\n",
    "    'Surface Air Cooler Water Outlet Temperature',\n",
    "    'Stator core temperature', 'UGB metal temperature',\n",
    "    'LGB metal temperature 1', 'LGB metal temperature 2',\n",
    "    'LGB oil temperature', 'Penstock Flow', 'Turbine flow',\n",
    "    'UGB cooling water flow', 'LGB cooling water flow',\n",
    "    'Generator cooling water flow', 'Governor Penstock Pressure',\n",
    "    'Penstock pressure', 'Opening Wicked Gate', 'UGB Oil Contaminant',\n",
    "    'Gen Thrust Bearing Oil Contaminant']\n",
    "\n",
    "with open('normalize_2023.pickle', 'rb') as handle:\n",
    "    normalize_obj = pickle.load(handle)\n",
    "    min_a, max_a = normalize_obj['min_a'], normalize_obj['max_a']\n",
    "min_a = min_a.astype(float)\n",
    "max_a = max_a.astype(float)\n",
    "\n",
    "model_array = [\"Attention\", \"DTAAD\", \"MAD_GAN\", \"TranAD\", \"DAGMM\", \"USAD\", \"OmniAnomaly\"]\n",
    "with open('model_thr.pickle', 'rb') as handle:\n",
    "    model_thr = pickle.load(handle)\n",
    "\n",
    "measured_horizon = 60 * 2 * 1\n",
    "\n",
    "init_db_timeconst(feature_set, \"db/original_data.db\", \"original_data\")\n",
    "init_db_timeconst(feature_set, \"db/severity_trendings.db\", \"severity_trendings\")\n",
    "init_db_timeconst(feature_set, \"db/severity_trendings.db\", \"original_sensor\")\n",
    "for model_name in model_array:\n",
    "    init_db_timeconst(feature_set, \"db/pred_data.db\", model_name)\n",
    "    init_db_timeconst(feature_set, \"db/threshold_data.db\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sensors = 30\n",
    "num_timesteps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing task... 0\n",
      "Executing task... 1\n",
      "Executing task... 2\n",
      "Executing task... 3\n",
      "Executing task... 4\n",
      "Executing task... 5\n",
      "Executing task... 6\n",
      "Executing task... 7\n",
      "Executing task... 8\n",
      "Executing task... 9\n",
      "Executing task... 10\n",
      "Executing task... 11\n",
      "Executing task... 12\n",
      "Executing task... 13\n",
      "Executing task... 14\n",
      "Executing task... 15\n",
      "Executing task... 16\n",
      "Executing task... 17\n",
      "Executing task... 18\n",
      "Executing task... 19\n",
      "Executing task... 20\n",
      "Executing task... 21\n",
      "Executing task... 22\n",
      "Executing task... 23\n",
      "Executing task... 24\n",
      "Executing task... 25\n",
      "Executing task... 26\n",
      "Executing task... 27\n",
      "Executing task... 28\n",
      "Executing task... 29\n",
      "Executing task... 30\n",
      "Executing task... 31\n",
      "Executing task... 32\n",
      "Executing task... 33\n",
      "Executing task... 34\n",
      "Executing task... 35\n",
      "Executing task... 36\n",
      "Executing task... 37\n",
      "Executing task... 38\n",
      "Executing task... 39\n",
      "Executing task... 40\n",
      "Executing task... 41\n",
      "Executing task... 42\n",
      "Executing task... 43\n",
      "Executing task... 44\n",
      "Executing task... 45\n",
      "Executing task... 46\n",
      "Executing task... 47\n",
      "Executing task... 48\n",
      "Executing task... 49\n",
      "Executing task... 50\n",
      "Executing task... 51\n",
      "Executing task... 52\n",
      "Executing task... 53\n",
      "Executing task... 54\n",
      "Executing task... 55\n",
      "Executing task... 56\n",
      "Executing task... 57\n",
      "Executing task... 58\n",
      "Executing task... 59\n",
      "Executing task... 60\n",
      "Executing task... 61\n",
      "Executing task... 62\n",
      "Executing task... 63\n",
      "Executing task... 64\n",
      "Executing task... 65\n",
      "Executing task... 66\n",
      "Executing task... 67\n",
      "Executing task... 68\n",
      "Executing task... 69\n",
      "Executing task... 70\n",
      "Executing task... 71\n",
      "Executing task... 72\n",
      "Executing task... 73\n",
      "Executing task... 74\n",
      "Executing task... 75\n",
      "Executing task... 76\n",
      "Executing task... 77\n",
      "Executing task... 78\n",
      "Executing task... 79\n",
      "Executing task... 80\n",
      "Executing task... 81\n",
      "Executing task... 82\n",
      "Executing task... 83\n",
      "Executing task... 84\n",
      "Executing task... 85\n",
      "Executing task... 86\n",
      "Executing task... 87\n",
      "Executing task... 88\n",
      "Executing task... 89\n",
      "Executing task... 90\n",
      "Executing task... 91\n",
      "Executing task... 92\n",
      "Executing task... 93\n",
      "Executing task... 94\n",
      "Executing task... 95\n",
      "Executing task... 96\n",
      "Executing task... 97\n",
      "Executing task... 98\n",
      "Executing task... 99\n",
      "Executing task... 100\n",
      "Executing task... 101\n",
      "Executing task... 102\n",
      "Executing task... 103\n",
      "Executing task... 104\n",
      "Executing task... 105\n",
      "Executing task... 106\n",
      "Executing task... 107\n",
      "Executing task... 108\n",
      "Executing task... 109\n",
      "Executing task... 110\n",
      "Executing task... 111\n",
      "Executing task... 112\n",
      "Executing task... 113\n",
      "Executing task... 114\n",
      "Executing task... 115\n",
      "Executing task... 116\n",
      "Executing task... 117\n",
      "Executing task... 118\n",
      "Executing task... 119\n",
      "Executing task... 120\n",
      "Executing task... 121\n",
      "Executing task... 122\n",
      "Executing task... 123\n",
      "Executing task... 124\n",
      "Executing task... 125\n",
      "Executing task... 126\n",
      "Executing task... 127\n",
      "Executing task... 128\n",
      "Executing task... 129\n",
      "Executing task... 130\n",
      "Executing task... 131\n",
      "Executing task... 132\n",
      "Executing task... 133\n",
      "Executing task... 134\n",
      "Executing task... 135\n",
      "Executing task... 136\n",
      "Executing task... 137\n",
      "Executing task... 138\n",
      "Executing task... 139\n",
      "Executing task... 140\n",
      "Executing task... 141\n",
      "Executing task... 142\n",
      "Executing task... 143\n",
      "Executing task... 144\n",
      "Executing task... 145\n",
      "Executing task... 146\n",
      "Executing task... 147\n",
      "Executing task... 148\n",
      "Executing task... 149\n",
      "Executing task... 150\n",
      "Executing task... 151\n",
      "Executing task... 152\n",
      "Executing task... 153\n",
      "Executing task... 154\n",
      "Executing task... 155\n",
      "Executing task... 156\n",
      "Executing task... 157\n",
      "Executing task... 158\n",
      "Executing task... 159\n",
      "Executing task... 160\n",
      "Executing task... 161\n",
      "Executing task... 162\n",
      "Executing task... 163\n",
      "Executing task... 164\n",
      "Executing task... 165\n",
      "Executing task... 166\n",
      "Executing task... 167\n",
      "Executing task... 168\n",
      "Executing task... 169\n",
      "Executing task... 170\n",
      "Executing task... 171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m df_timestamp_last \u001b[38;5;241m=\u001b[39m df_timestamp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     62\u001b[0m count \u001b[38;5;241m=\u001b[39m count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 63\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m180\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "conn = sqlite3.connect(\"db/original_data.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"\"\"SELECT * FROM original_data order by rowid desc LIMIT 1\"\"\")\n",
    "rows = cursor.fetchall()\n",
    "conn.close()\n",
    "df_timestamp_last = np.datetime64(np.array(rows)[:, 1][0])  #np.datetime64('now') - np.timedelta64(2, 'h') #np.datetime64(np.array(rows)[:, 1][0]) \n",
    "while True:\n",
    "    print(\"Executing task... \" + str(count))\n",
    "\n",
    "    df_feature = np.zeros((num_timesteps, num_sensors))\n",
    "    temp_ypreds = []\n",
    "    for idx, _ in enumerate(model_array):\n",
    "        temp_ypreds.append(np.zeros((num_timesteps, num_sensors)))\n",
    "\n",
    "    df_feature[0] = np.random.uniform(min_a, max_a)\n",
    "    for idx, _ in enumerate(model_array):\n",
    "        temp_ypreds[idx][0] = df_feature[0].copy()\n",
    "\n",
    "    for t in range(1, num_timesteps):\n",
    "        prev_values = df_feature[t - 1]\n",
    "        perturbation = np.random.uniform(-0.05, 0.05, size=(num_sensors)) * prev_values  # Max 5% change\n",
    "        new_values = prev_values + perturbation\n",
    "        new_values = np.clip(new_values, min_a, max_a)  # Ensure within bounds\n",
    "        df_feature[t] = new_values\n",
    "        for idx, _ in enumerate(model_array):\n",
    "            temp_ypreds[idx][t] = (np.random.uniform(-0.1, 0.1, size=(num_sensors)) * new_values) + new_values\n",
    "\n",
    "    df_feature_mean = trunc(np.mean(df_feature, axis=0), decs=2)\n",
    "    trend_data = np.random.poisson(5, num_sensors)\n",
    "\n",
    "    threshold_percentages = {}\n",
    "    for idx, _ in enumerate(model_array):\n",
    "        temp_thre = {}\n",
    "        for feat_name in feature_set:\n",
    "            temp_thre[feat_name] = int(np.random.uniform(0, 7))\n",
    "        threshold_percentages[idx] = temp_thre\n",
    "\n",
    "    now_time = np.datetime64('now')\n",
    "    past_time = now_time - np.timedelta64(2, 'h')\n",
    "    df_timestamp = np.arange(past_time, now_time, np.timedelta64(6, 'm'))\n",
    "\n",
    "    mask = df_timestamp > df_timestamp_last\n",
    "    df_feature = df_feature[mask]\n",
    "    df_timestamp = df_timestamp[mask]\n",
    "    for i in range(len(model_array)):\n",
    "        temp_ypreds[i] = temp_ypreds[i][mask]\n",
    "\n",
    "    batch_timeseries_savedb(df_timestamp, trunc(df_feature, decs=2), feature_set, \"db/original_data.db\", \"original_data\")\n",
    "    for idx_model, (model_name) in enumerate(model_array):\n",
    "        batch_timeseries_savedb(df_timestamp, trunc(temp_ypreds[idx_model], decs=2), feature_set, \"db/pred_data.db\", model_name) \n",
    "\n",
    "    df_timestampi = pd.to_datetime(df_timestamp[-1])\n",
    "    for model_idx, model_name in enumerate(model_array):\n",
    "        timeseries_savedb(df_timestampi, trunc(np.array(list(threshold_percentages[model_idx].values())), decs=2), feature_set, \"db/threshold_data.db\", model_name) \n",
    "\n",
    "    timeseries_savedb(df_timestampi, trend_data.tolist(), feature_set, \"db/severity_trendings.db\", \"severity_trendings\") \n",
    "    timeseries_savedb(df_timestampi, df_feature_mean, feature_set, \"db/severity_trendings.db\", \"original_sensor\") \n",
    "\n",
    "    # DONT REMOVE THIS\n",
    "    df_timestamp_last = df_timestamp[-1]\n",
    "    count = count + 1\n",
    "    time.sleep(180)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
