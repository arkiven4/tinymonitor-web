{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tokenizers import PreTokenizedString\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.stats import spearmanr\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "from django.conf import settings\n",
    "import apis.commons as commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tag_mapping = {\n",
    "    'LGS1 Active Power': 'U-LGS1-Active-Power-AI',\n",
    "    'LGS1-Auxiliary Grid (0 = ACTIVE)': 'U-LGS1-N75-15-0-AI',\n",
    "    'Unit Breaker A1G1 Closed': 'U-LGS1-Gen-CB-Closed-DI',\n",
    "    'Unit Breaker A1G1 Open': 'U-LGS1-Gen-CB-Open-DI',\n",
    "    'LGS1 Governor Unit Speed Actual': 'U-LGS1-SI-81101-AI',\n",
    "    \n",
    "    'LGS2 Active Power': 'U-LGS2-Active-Power-AI',\n",
    "    'LGS2-Auxiliary Grid (0 = ACTIVE)': 'U-LGS2-N75-25-0-AI',\n",
    "    'Unit Breaker A1G2 Closed': 'U-LGS2-Gen-CB-Closed-DI',\n",
    "    'Unit Breaker A1G2 Open': 'U-LGS2-Gen-CB-Open-DI',\n",
    "    'LGS2 Governor Unit Speed Actual': 'U-LGS2-SI-81201-AI',\n",
    "    \n",
    "    'LGS3 Active Power': 'U-LGS3_Active-Power-AI',\n",
    "    'LGS3-Auxiliary Grid (0 = ACTIVE)': 'U-LGS3-N75-35-0-AI',\n",
    "    'Unit Breaker A1G3 Closed': 'U-LGS3_Gen-CB-Closed-DI',\n",
    "    'Unit Breaker A1G3 Open': 'U-LGS3_Gen-CB-Open-DI',\n",
    "    'LGS3 Governor Unit Speed Actual': 'U-LGS3_SI_81301_I_Eng-AI',\n",
    "\n",
    "    'Avg Hydro Power Available 1D (Avg)': 'U-PWR-HYDRO-AI-AVGD',\n",
    "    'Total Hydro Power Daily (Tot)': 'U-HGST-Power-AI-DTT',\n",
    "    'Total Larona Power Daily (Tot)': 'U-PWR-LAR-TOT-DTT',\n",
    "    'Total Balambano Power Daily (Tot)': 'U-PWR-BAL-TOT-DTT',\n",
    "\n",
    "    # BGS\n",
    "    'BGS1 Power': 'U-BGS1-Power-AI',\n",
    "    'BGS1-Auxiliary Grid (0 = ACTIVE)': 'U-BGS1-N75-45-0-AI',\n",
    "    'Unit breaker BGS1': 'U-BGS1-Brk-DI',\n",
    "    'GEN SPEED BGS1': 'U-BGS1_I_T_SPEED-AI',\n",
    "\n",
    "    'BGS2 Power': 'U-BGS2-Power-AI',\n",
    "    'BGS2-Auxiliary Grid (0 = ACTIVE)': 'U-BGS2-N75-55-0-AI',\n",
    "    'Unit breaker BGS2': 'U-BGS2-Brk-DI',\n",
    "    'GEN SPEED BGS2': 'U-BGS2_I_T_SPEED-AI',\n",
    "\n",
    "    # KGS\n",
    "    'K U1 Active Power (MW)': 'U-KGS1-Active_Power_AI',\n",
    "    'KGS1-Auxiliary Grid (0 = ACTIVE)': 'U-KGS1-N75-65-0-AI',\n",
    "    'K U1 11kV Gen Circuit 52G1 Breaker Closed': 'U-KGS1-11KV-Gen_CB_closed-DI',\n",
    "    'K U1 11kV Gen Circuit 52G1 Breaker open': 'U-KGS1-11KV-Gen_CB_open-DI',\n",
    "    'K U1 Turb Gov Turbine Speed (RPM)': 'U-KGS1-Turb_Gov_Turb_Speed-AI',\n",
    "\n",
    "    'K U2 Active Power (MW)': 'U-KGS2-Active_Power_AI',\n",
    "    'KGS2-Auxiliary Grid (0 = ACTIVE)': 'U-KGS2-N75-75-0-AI',\n",
    "    'K U2 11kV Gen Circuit 52G2 Breaker Closed': 'U-KGS2-11KV-Gen_CB_closed-DI',\n",
    "    'K U2 11kV Gen Circuit 52G2 Breaker open': 'U-KGS2-11KV-Gen_CB_open-DI',\n",
    "    'K U2 Turb Gov Turbine Speed (RPM)': 'U-KGS2-Turb_Gov_Turb_Speed-AI',\n",
    "}\n",
    "\n",
    "reverse_mapping = {v: k for k, v in feature_tag_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "master_pd = \"\"\n",
    "column_name = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(\"data_csv/2024\"):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        tag_name = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "        feature_key = reverse_mapping.get(tag_name)\n",
    "        column_name.append(feature_key)\n",
    "\n",
    "        value_resp = pd.read_csv(filepath)\n",
    "        if count == 0:\n",
    "            value_resp['Timestamps'] = pd.to_datetime(value_resp['Timestamps'])\n",
    "            master_pd = value_resp\n",
    "        else:\n",
    "            master_pd = pd.concat([master_pd, value_resp['Values']], axis=1, join='inner')\n",
    "\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_pd = master_pd.values\n",
    "master_pd = pd.DataFrame(data=master_pd, columns=['TimeStamp'] + list(column_name)) #+ feature_set + ['Grid Selection'])\n",
    "master_pd = master_pd.reset_index(drop=True)\n",
    "master_pd.replace('I/O Timeout', np.nan, inplace=True)\n",
    "master_pd.replace('No Data', np.nan, inplace=True)\n",
    "master_pd.replace('Future Data Unsupported', np.nan, inplace=True)\n",
    "master_pd.replace('Closed', np.nan, inplace=True)\n",
    "master_pd.replace('Open', np.nan, inplace=True)\n",
    "\n",
    "for column_name in master_pd.columns:\n",
    "    if column_name != 'Load_Type' and column_name != 'TimeStamp':\n",
    "        master_pd[column_name] = pd.to_numeric(master_pd[column_name], downcast='float')\n",
    "\n",
    "master_pd = master_pd.sort_values(by='TimeStamp')\n",
    "master_pd = master_pd.reset_index(drop=True)\n",
    "master_pd = master_pd.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_pd.drop(['Unit breaker BGS2', 'Unit breaker BGS1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_pd.to_csv(\"kpi2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_db_timeconst(feature_set, db_name=\"masters_data.db\", table_name=\"severity_trending\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table if it does not exist\n",
    "    columns = \", \".join([feature_name.replace(\" \", \"_\") for feature_name in feature_set])\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            timestamp TEXT,\n",
    "            {columns}\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def timeseries_savedb(df_timestamp, data, feature_set, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    #if len(data) == 30:\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    # Generate timestamp\n",
    "    timestamp = df_timestamp.isoformat()\n",
    "    \n",
    "    # Build column names for features, replacing spaces with underscores\n",
    "    feature_columns = ', '.join([feature_name.replace(\" \", \"_\") for feature_name in feature_set])\n",
    "    placeholders = ', '.join(['?' for _ in range(len(feature_set))])\n",
    "    \n",
    "    # Upsert using INSERT OR REPLACE\n",
    "    # Note: Your table must have a UNIQUE constraint on the timestamp column.\n",
    "    sql = f\"\"\"\n",
    "        INSERT OR REPLACE INTO {table_name} (timestamp, {feature_columns})\n",
    "        VALUES (?, {placeholders})\n",
    "    \"\"\"\n",
    "    cursor.execute(sql, (timestamp, *data))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def is_number(x):\n",
    "    return isinstance(x, (int, float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_db_timeconst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m plant_metadata\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value2 \u001b[38;5;129;01min\u001b[39;00m value:\n\u001b[0;32m---> 55\u001b[0m         \u001b[43minit_db_timeconst\u001b[49m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moee\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphy_avail\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperformance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muo_Avail\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maux_0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maux_1\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb/kpi.db\u001b[39m\u001b[38;5;124m\"\u001b[39m, value2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     57\u001b[0m init_db_timeconst([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlpd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhpd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbpd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mahpa\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb/kpi.db\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPowerProd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_db_timeconst' is not defined"
     ]
    }
   ],
   "source": [
    "plant_metadata = {\n",
    "    'Larona': [{\n",
    "        'name': \"LGS1\",\n",
    "        'active_power': 'LGS1 Active Power',\n",
    "        'rpm': 'LGS1 Governor Unit Speed Actual',\n",
    "        'aux': 'LGS1-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [20.944, 11.398]\n",
    "    },\n",
    "    {\n",
    "        'name': \"LGS2\",\n",
    "        'active_power': 'LGS2 Active Power',\n",
    "        'rpm': 'LGS2 Governor Unit Speed Actual',\n",
    "        'aux': 'LGS2-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [21.162, 8.49]\n",
    "    },\n",
    "    {\n",
    "        'name': \"LGS3\",\n",
    "        'active_power': 'LGS3 Active Power',\n",
    "        'rpm': 'LGS3 Governor Unit Speed Actual',\n",
    "        'aux': 'LGS3-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [19.66, 13.676]\n",
    "    }],\n",
    "    'Balambano': [{\n",
    "        'name': \"BGS1\",\n",
    "        'active_power': 'BGS1 Power',\n",
    "        'rpm': 'GEN SPEED BGS1',\n",
    "        'aux': 'BGS1-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [20.944, 11.398]\n",
    "    },\n",
    "    {\n",
    "        'name': \"BGS2\",\n",
    "        'active_power': 'BGS2 Power',\n",
    "        'rpm': 'GEN SPEED BGS2',\n",
    "        'aux': 'BGS2-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [21.162, 8.49]\n",
    "    }],\n",
    "    'Karebbe': [{\n",
    "        'name': \"KGS1\",\n",
    "        'active_power': 'K U1 Active Power (MW)',\n",
    "        'rpm': 'K U1 Turb Gov Turbine Speed (RPM)',\n",
    "        'aux': 'KGS1-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [20.944, 11.398]\n",
    "    },\n",
    "    {\n",
    "        'name': \"KGS2\",\n",
    "        'active_power': 'K U2 Active Power (MW)',\n",
    "        'rpm': 'K U2 Turb Gov Turbine Speed (RPM)',\n",
    "        'aux': 'KGS2-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [21.162, 8.49]\n",
    "    }]\n",
    "}\n",
    "\n",
    "for value in plant_metadata.values():\n",
    "    for value2 in value:\n",
    "        init_db_timeconst(['oee', 'phy_avail', 'performance', 'uo_Avail', \"aux_0\", \"aux_1\"], \"db/kpi.db\", value2['name'])\n",
    "\n",
    "init_db_timeconst(['lpd', 'hpd', 'bpd', 'ahpa'], \"db/kpi.db\", \"PowerProd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lohe = []\n",
    "for value in plant_metadata.values():\n",
    "    for value2 in value:\n",
    "        lohe.append(value2['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LGS1', 'LGS2', 'LGS3', 'BGS1', 'BGS2', 'KGS1', 'KGS2']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shutdown_and_snl_periods(df_selected, column_name):\n",
    "    data_timestamp = df_selected[['TimeStamp']].values\n",
    "    sensor_datas = df_selected[column_name].values\n",
    "\n",
    "    activepower_data = sensor_datas[:, 0].astype(float)\n",
    "    rpm_data = sensor_datas[:, 1].astype(float)\n",
    "\n",
    "    shutdown_mask = (activepower_data <= 3) & (rpm_data <= 10)\n",
    "    snl_mask = (activepower_data <= 3) & (rpm_data >= 259.35) & (rpm_data <= 286.65)\n",
    "\n",
    "    def extract_periods(mask):\n",
    "        change_points = np.diff(mask.astype(int), prepend=0)\n",
    "        start_indices = np.where(change_points == 1)[0]\n",
    "        end_indices = np.where(change_points == -1)[0]\n",
    "\n",
    "        if mask[-1]:\n",
    "            end_indices = np.append(end_indices, len(mask))\n",
    "        if mask[0]:\n",
    "            start_indices = np.insert(start_indices, 0, 0)\n",
    "\n",
    "        periods = []\n",
    "        for start, end in zip(start_indices, end_indices):\n",
    "            start_time = data_timestamp[start][0]\n",
    "            end_time = data_timestamp[end - 1][0]\n",
    "            periods.append((start_time, end_time))\n",
    "        return periods\n",
    "\n",
    "    shutdown_periods = extract_periods(shutdown_mask)\n",
    "    snl_periods = extract_periods(snl_mask)\n",
    "\n",
    "    return shutdown_periods, snl_periods\n",
    "\n",
    "def compute_oee_metrics(df_selected, column_name, shutdown_periods, snl_periods, performance_formula):\n",
    "    data_timestamp = df_selected[['TimeStamp']].values.flatten()\n",
    "    sensor_datas = df_selected[column_name].values\n",
    "\n",
    "    active_power = sensor_datas[:, 0].astype(float)\n",
    "\n",
    "    nonzeroneg_mask = active_power > 0\n",
    "    total_hours = (pd.to_datetime(str(data_timestamp[-1])) - pd.to_datetime(str(data_timestamp[0]))).total_seconds() / 3600\n",
    "\n",
    "    downtime_hours = sum(\n",
    "        (pd.to_datetime(str(end)) - pd.to_datetime(str(start))).total_seconds() / 3600\n",
    "        for start, end in shutdown_periods\n",
    "    )\n",
    "    snl_hours = sum(\n",
    "        (pd.to_datetime(str(end)) - pd.to_datetime(str(start))).total_seconds() / 3600\n",
    "        for start, end in snl_periods\n",
    "    )\n",
    "\n",
    "    phy_avail = max(round((total_hours - downtime_hours) / total_hours, 2), 0.01)\n",
    "    uo_Avail = max(round((total_hours - snl_hours) / total_hours, 2), 0.01)\n",
    "\n",
    "    if np.any(nonzeroneg_mask):\n",
    "        log_mean = np.mean(np.log(active_power[nonzeroneg_mask]))\n",
    "        performance = max(round((performance_formula[0] * log_mean + performance_formula[1]) / 100, 2), 0)\n",
    "    else:\n",
    "        performance = 0.01\n",
    "\n",
    "    oee = max(round(phy_avail * performance * uo_Avail, 2), 0.01)\n",
    "    datetime_nowMidnight = pd.to_datetime(str(data_timestamp[-1])).replace(hour=1, minute=0, second=0)\n",
    "\n",
    "    return datetime_nowMidnight, oee, phy_avail, performance, uo_Avail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.to_datetime('2024-01-01 01:00:00')\n",
    "end_time = master_pd['TimeStamp'].max()\n",
    "\n",
    "current_start = start_time\n",
    "while current_start < end_time:\n",
    "    current_end = current_start + pd.DateOffset(days=1)\n",
    "\n",
    "    mask = (master_pd['TimeStamp'] >= current_start) & (\n",
    "        master_pd['TimeStamp'] < current_end)\n",
    "    df_sel = master_pd.loc[mask]\n",
    "\n",
    "    for value in plant_metadata.values():\n",
    "        for tags in value:\n",
    "            unit_name = tags['name']\n",
    "\n",
    "            if tags['active_power'] not in df_sel.columns or tags['rpm'] not in df_sel.columns:\n",
    "                continue  # Skip if required data not present\n",
    "\n",
    "            df_unit = df_sel[['TimeStamp', tags['active_power'], tags['rpm'], tags['aux']]].dropna()\n",
    "            if df_unit.empty:\n",
    "                continue\n",
    "\n",
    "            # Process shutdown & SNL\n",
    "            shutdown_periods, snl_periods = process_shutdown_and_snl_periods(\n",
    "                df_unit, [tags['active_power'], tags['rpm']]\n",
    "            )\n",
    "\n",
    "            # Compute OEE and related KPIs\n",
    "            datetime_nowMidnight, oee, phy_avail, performance, uo_Avail = compute_oee_metrics(\n",
    "                df_unit, [tags['active_power'], tags['rpm']],\n",
    "                shutdown_periods, snl_periods,\n",
    "                performance_formula=tags['coef']\n",
    "            )\n",
    "\n",
    "            # Count Auxiliary Grid ON/OFF\n",
    "            counts_aux = df_unit[tags['aux']].value_counts().sort_index()\n",
    "            aux_0 = counts_aux.get(0.0, 0)\n",
    "            aux_1 = counts_aux.get(1.0, 0)\n",
    "\n",
    "            # Save to database\n",
    "            timeseries_savedb(\n",
    "                datetime_nowMidnight,\n",
    "                np.array([oee, phy_avail, performance, uo_Avail, aux_0, aux_1]),\n",
    "                ['oee', 'phy_avail', 'performance', 'uo_Avail', 'aux_0', 'aux_1'],\n",
    "                \"db/kpi.db\",\n",
    "                unit_name\n",
    "            )\n",
    "\n",
    "    pda_datas = df_sel[['Total Larona Power Daily (Tot)', 'Total Hydro Power Daily (Tot)', 'Total Balambano Power Daily (Tot)', 'Avg Hydro Power Available 1D (Avg)']].mean().values\n",
    "    timeseries_savedb(\n",
    "            datetime_nowMidnight,\n",
    "            np.array([pda_datas[0], pda_datas[1], pda_datas[2], pda_datas[3]]).astype(np.float64),\n",
    "            ['lpd', 'hpd', 'bpd', 'ahpa'],\n",
    "            \"db/kpi.db\",\n",
    "            \"PowerProd\"\n",
    "        )\n",
    "    current_start = current_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_datas = commons.fetch_between_dates(\"2023-01-22T18:13:00\", \"2023-05-22T19:52:00\", \"db/original_data.db\", \"original_data\")\n",
    "data_timestamp = sensor_datas[:, 1]\n",
    "sensor_datas = sensor_datas[:, 2:].astype(float)\n",
    "\n",
    "frame_len = 286\n",
    "num_frames = len(sensor_datas) // frame_len  # 120\n",
    "usable_len = frame_len * num_frames  # 34320\n",
    "\n",
    "data_timestamp = data_timestamp[:usable_len]\n",
    "sensor_datas = sensor_datas[:usable_len]\n",
    "\n",
    "frames_timestamp = data_timestamp.reshape(num_frames, frame_len)\n",
    "frames_sensor = sensor_datas.reshape(num_frames, frame_len, 30)\n",
    "for i_frame, frame in enumerate(frames_sensor):\n",
    "    now_timestamp = frames_timestamp[i_frame, :]\n",
    "    now_sensors = frames_sensor[i_frame, :]\n",
    "\n",
    "    shutdown_periods = commons.process_shutdownTimestamp(now_timestamp, now_sensors)\n",
    "    snl_periods = commons.process_SNLTimestamp(now_timestamp, now_sensors)\n",
    "\n",
    "    nonzeroneg_activepower = now_sensors[:, 0] > 0\n",
    "    total_hours = (datetime.fromisoformat(str(now_timestamp[-1])) - datetime.fromisoformat(str(now_timestamp[0]))).total_seconds() / 3600\n",
    "    downtime_hours = 0\n",
    "    for datespan_downtime in shutdown_periods:\n",
    "        delta = datetime.fromisoformat(str(datespan_downtime[1])) - datetime.fromisoformat(str(datespan_downtime[0]))\n",
    "        delta_hours = delta.total_seconds() / 3600\n",
    "        downtime_hours += delta_hours\n",
    "\n",
    "    snl_hours = 0\n",
    "    for datespan_snl in snl_periods:\n",
    "        delta = datetime.fromisoformat(str(datespan_snl[1])) - datetime.fromisoformat(str(datespan_snl[0]))\n",
    "        delta_hours = delta.total_seconds() / 3600\n",
    "        snl_hours += delta_hours\n",
    "\n",
    "    phy_avail = max(round(((total_hours - downtime_hours) / total_hours), 2), 0.01)\n",
    "    uo_Avail = max(round(((total_hours - snl_hours) / total_hours), 2), 0.01)\n",
    "    if len(now_sensors[nonzeroneg_activepower, 0]) > 0:\n",
    "        performance = max(round((20.944 * np.mean(np.log(now_sensors[nonzeroneg_activepower, 0])) + 11.398)/100, 2), 0)\n",
    "    else:\n",
    "        performance = 0.01\n",
    "    \n",
    "    oee = max(round(phy_avail * performance * uo_Avail,2), 0.01)\n",
    "    datetime_nowMidnight = datetime.fromisoformat(str(now_timestamp[-1])).replace(hour=1, minute=0, second=0)\n",
    "\n",
    "    timeseries_savedb(datetime_nowMidnight, np.array([oee, phy_avail, performance, uo_Avail]), ['oee', 'phy_avail', 'performance', 'uo_Avail'], \"db/kpi.db\", \"kpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_datas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "286 / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "from django.conf import settings\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "feature_set = ['Active Power', 'Reactive Power', 'Governor speed actual', 'UGB X displacement', 'UGB Y displacement',\n",
    "    'LGB X displacement', 'LGB Y displacement', 'TGB X displacement',\n",
    "    'TGB Y displacement', 'Stator winding temperature 13',\n",
    "    'Stator winding temperature 14', 'Stator winding temperature 15',\n",
    "    'Surface Air Cooler Air Outlet Temperature',\n",
    "    'Surface Air Cooler Water Inlet Temperature',\n",
    "    'Surface Air Cooler Water Outlet Temperature',\n",
    "    'Stator core temperature', 'UGB metal temperature',\n",
    "    'LGB metal temperature 1', 'LGB metal temperature 2',\n",
    "    'LGB oil temperature', 'Penstock Flow', 'Turbine flow',\n",
    "    'UGB cooling water flow', 'LGB cooling water flow',\n",
    "    'Generator cooling water flow', 'Governor Penstock Pressure',\n",
    "    'Penstock pressure', 'Opening Wicked Gate', 'UGB Oil Contaminant',\n",
    "    'Gen Thrust Bearing Oil Contaminant']\n",
    "\n",
    "model_array = [\"Attention\", \"DTAAD\", \"MAD_GAN\", \"TranAD\", \"DAGMM\", \"USAD\", \"OmniAnomaly\"]\n",
    "with open('model_thr.pickle', 'rb') as handle:\n",
    "    model_thr = pickle.load(handle)\n",
    "\n",
    "with open('normalize_2023.pickle', 'rb') as handle:\n",
    "    normalize_obj = pickle.load(handle)\n",
    "    min_a, max_a = normalize_obj['min_a'], normalize_obj['max_a']\n",
    "\n",
    "def normalize3(a, min_a=None, max_a=None):\n",
    "    if min_a is None: min_a, max_a = np.min(a, axis=0), np.max(a, axis=0)\n",
    "    return ((a - min_a) / (max_a - min_a + 0.0001)), min_a, max_a\n",
    "\n",
    "def denormalize3(a_norm, min_a, max_a):\n",
    "    return a_norm * (max_a - min_a + 0.0001) + min_a\n",
    "\n",
    "\n",
    "def fetch_between_dates(start_date, end_date, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT * FROM {table_name} WHERE timestamp BETWEEN ? AND ?\n",
    "    \"\"\", (start_date, end_date))\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not rows:\n",
    "        return np.array([])\n",
    "    \n",
    "    return np.array(rows)\n",
    "\n",
    "def fetch_last_rows(num_row, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT * FROM {table_name} ORDER BY timestamp DESC LIMIT ?\n",
    "    \"\"\", (num_row,))\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not rows:\n",
    "        return np.array([])\n",
    "    \n",
    "    return np.array(rows)\n",
    "\n",
    "def convert_timestamp(timestamp_str):\n",
    "    dt = datetime.fromisoformat(timestamp_str)\n",
    "    return pd.Timestamp(dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def percentage2severity(value):\n",
    "    return (\n",
    "        1 if 0 <= value < 5 else\n",
    "        2 if 5 <= value < 20 else\n",
    "        3 if 20 <= value < 40 else\n",
    "        4 if 40 <= value < 75 else\n",
    "        5 if 75 <= value <= 100 else\n",
    "        6\n",
    "    )\n",
    "    \n",
    "def calc_counterPercentage(threshold_percentages):\n",
    "    counter_feature = {}\n",
    "    for modex_idx, values_pred in threshold_percentages.items():\n",
    "        values_pred = dict(sorted(values_pred.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "        for name_feat, percentage in values_pred.items():\n",
    "            if name_feat in counter_feature:\n",
    "                counter_feature[name_feat][\"count\"] = counter_feature[name_feat][\"count\"] + 1\n",
    "                counter_feature[name_feat][\"percentage\"] = counter_feature[name_feat][\"percentage\"] + percentage\n",
    "            else:\n",
    "                counter_feature[name_feat] = {\"count\": 1, \"percentage\": percentage}\n",
    "\n",
    "    counter_feature_s1 = dict(sorted(counter_feature.items(), key=lambda item: item[1]['count'], reverse=True)[:10])\n",
    "    counter_feature_s2 = dict(sorted(counter_feature_s1.items(), key=lambda item: item[1]['percentage'] // len(model_array), reverse=True))\n",
    "    #counter_feature_s2_rank = dict(sorted(counter_feature_s1.items(), key=lambda item: item[1]['count'], reverse=True))\n",
    "\n",
    "    for key, value in counter_feature_s2.items():\n",
    "        counter_feature_s2[key]['count'] = (counter_feature_s2[key]['count'] / len(model_array)) * 100\n",
    "        counter_feature_s2[key]['severity'] = percentage2severity(counter_feature_s2[key]['percentage'] // len(model_array))\n",
    "        counter_feature_s2[key]['percentage'] = (counter_feature_s2[key]['percentage'] // len(model_array))\n",
    "\n",
    "    # Find Which Model Have Highest Confidence\n",
    "    counter_feature_plot = {}\n",
    "    for index, value in counter_feature_s2.items():\n",
    "        higher_data = {\"model\": 0, \"percentage\": 0}\n",
    "        for model_idx in threshold_percentages:\n",
    "            if index in threshold_percentages[model_idx]:\n",
    "                if higher_data[\"percentage\"] <= threshold_percentages[model_idx][index]:\n",
    "                    higher_data[\"model\"] = model_idx\n",
    "                    higher_data[\"percentage\"] = threshold_percentages[model_idx][index]\n",
    "        \n",
    "        counter_feature_plot[index] = higher_data['model']\n",
    "\n",
    "    return counter_feature_s2, counter_feature_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set.index(\"LGB cooling water flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hampel_filter(series, window_size=3, n_sigmas=3):\n",
    "    new_series = series.copy()\n",
    "    k = 1.4826  # scale factor for Gaussian distribution\n",
    "    n = len(series)\n",
    "\n",
    "    for i in range(window_size, n - window_size):\n",
    "        window = series[i - window_size:i + window_size + 1]\n",
    "        median = np.median(window)\n",
    "        mad = k * np.median(np.abs(window - median))\n",
    "        if np.abs(series[i] - median) > n_sigmas * mad:\n",
    "            new_series[i] = median\n",
    "    return new_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = \"2021-05-28T05:55:00\"\n",
    "start_date = \"2021-04-28T06:10:00\"\n",
    "\n",
    "severity_trending_datas = fetch_between_dates(start_date, end_date, \"db/severity_trendings.db\", \"severity_trendings\")\n",
    "sensor_datas = fetch_between_dates(start_date, end_date, \"db/severity_trendings.db\", \"original_sensor\")\n",
    "    \n",
    "data_timestamp = sensor_datas[:, 1]\n",
    "severity_trending_datas = severity_trending_datas[:, 2:].astype(float)\n",
    "sensor_datas = sensor_datas[:, 2:].astype(float)\n",
    "\n",
    "for i in range(len(feature_set)):\n",
    "    severity_trending_datas[:, i] = hampel_filter(severity_trending_datas[:, i], window_size=300, n_sigmas=10)\n",
    "\n",
    "datetime_index = pd.to_datetime(data_timestamp)\n",
    "series = pd.Series(severity_trending_datas[:, 23], index=datetime_index)\n",
    "series = series.asfreq('15min')\n",
    "\n",
    "result = seasonal_decompose(series, model='additive', period=96 * 2)\n",
    "result.plot()\n",
    "\n",
    "trend = result.trend.dropna()\n",
    "\n",
    "x = np.arange(len(trend))\n",
    "corr, _ = spearmanr(x, trend)\n",
    "print(\"Spearman correlation:\", corr)\n",
    "\n",
    "if corr > 0.3:\n",
    "    priority = 1  # increasing\n",
    "elif corr < -0.3:\n",
    "    priority = 3  # decreasing\n",
    "else:\n",
    "    priority = 2  # flat\n",
    "\n",
    "priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def extract_time_series_features(x):\n",
    "    x = np.asarray(x)\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Basic statistics\n",
    "    features['mean'] = np.mean(x)\n",
    "    features['min'] = np.min(x)\n",
    "    features['max'] = np.max(x)\n",
    "    features['std'] = np.std(x)\n",
    "    features['range'] = np.ptp(x)  # max - min\n",
    "    features['mad'] = np.mean(np.abs(x - np.mean(x)))  # Mean Absolute Deviation\n",
    "    features['rms'] = np.sqrt(np.mean(x**2))  # Root Mean Square\n",
    "    features['skewness'] = skew(x)\n",
    "    features['kurtosis'] = kurtosis(x)\n",
    "\n",
    "    # Monotonicity (fraction of increasing / decreasing)\n",
    "    diffs = np.diff(x)\n",
    "    features['monotonic_increasing'] = np.sum(diffs > 0) / len(diffs)\n",
    "    features['monotonic_decreasing'] = np.sum(diffs < 0) / len(diffs)\n",
    "\n",
    "    # Slope (linear trend) and gradient\n",
    "    if len(x) > 1:\n",
    "        t = np.arange(len(x))\n",
    "        slope = np.polyfit(t, x, 1)[0]\n",
    "        features['slope'] = slope\n",
    "    else:\n",
    "        features['slope'] = np.nan\n",
    "\n",
    "    features['gradient_mean'] = np.mean(np.gradient(x))\n",
    "\n",
    "    df = pd.DataFrame([features])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 0.2\n",
    "window_size = 1492\n",
    "step_size = int(window_size * (1 - overlap))\n",
    "\n",
    "for start in range(0, len(severity_trending_datas) - window_size + 1, step_size):\n",
    "    #windows.append(data[start:start + window_size])\n",
    "    print(start, start + window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.gradient(severity_trending_datas[:, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(severity_trending_datas[:, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "data_pca = pca.fit_transform(severity_trending_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_pca[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trending.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "current_trending = severity_trending_datas[:, 8] # 8 22\n",
    "current_trending = np.convolve(current_trending, kernel, mode='same')\n",
    "\n",
    "y = (current_trending - 0) / (100 - 0)\n",
    "x = np.linspace(0, 30, len(y)).reshape(-1, 1) \n",
    "\n",
    "reg1 = LinearRegression()\n",
    "reg1.fit(x, y)\n",
    "\n",
    "y_linear1 = reg1.predict(x)\n",
    "\n",
    "plt.plot(x, y, label=\"Unknown Function\", color='blue')\n",
    "plt.plot(x, y_linear1, label=\"Linear Regression Fit\", linestyle=\"dashed\", color='orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "reg1.coef_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = LinearRegression()\n",
    "reg2 = LinearRegression()\n",
    "\n",
    "x = np.linspace(0, 10, 100).reshape(-1, 1) \n",
    "y = (1 / (0.001 + np.exp(-(x - 5))))\n",
    "y_rev = 1- (1 / (1 + np.exp(-(x - 5))))\n",
    "\n",
    "reg1.fit(x, y)\n",
    "reg2.fit(x, y_rev)\n",
    "\n",
    "y_linear1 = reg1.predict(x)\n",
    "y_linear2 = reg2.predict(x)\n",
    "\n",
    "m1 = reg1.coef_[0]\n",
    "m2 = reg2.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, label=\"Unknown Function\", color='blue')\n",
    "plt.plot(x, y_linear1, label=\"Linear Regression Fit\", linestyle=\"dashed\", color='orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y_rev, label=\"Unknown Function\", color='blue')\n",
    "plt.plot(x, y_linear2, label=\"Linear Regression Fit\", linestyle=\"dashed\", color='orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = LinearRegression()\n",
    "reg2 = LinearRegression()\n",
    "\n",
    "x = np.linspace(0, 10, 100).reshape(-1, 1) \n",
    "y = 10 * x\n",
    "y_rev = -10 * x\n",
    "\n",
    "reg1.fit(x, y)\n",
    "reg2.fit(x, y_rev)\n",
    "\n",
    "y_linear1 = reg1.predict(x)\n",
    "y_linear2 = reg2.predict(x)\n",
    "\n",
    "m1 = reg1.coef_[0]\n",
    "m2 = reg2.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg.fit(x, y)  # Fit y = mx + b\n",
    "m = reg.coef_[0]  # Extract slope (gradient)\n",
    "b = reg.intercept_  # Extract intercept\n",
    "\n",
    "# Compute predicted values from regression\n",
    "y_linear = reg.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 100)\n",
    "positive_gradients = [0.5 * x, x, 10 * x]  # Increasing slopes\n",
    "stationary_gradient = np.zeros_like(x)  # Flat line\n",
    "negative_gradients = [-0.5 * x, -x, -2 * x]  # Decreasing slopes\n",
    "\n",
    "# Define labels\n",
    "labels = ['Positive (0.5x)', 'Positive (x)', 'Positive (2x)',\n",
    "          'Stationary (0)',\n",
    "          'Negative (-0.5x)', 'Negative (-x)', 'Negative (-2x)']\n",
    "\n",
    "# Define colors\n",
    "colors = ['green', 'lime', 'darkgreen', 'black', 'red', 'orange', 'darkred']\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot positive gradients\n",
    "for i, y in enumerate(positive_gradients):\n",
    "    plt.plot(x, y, label=labels[i], color=colors[i])\n",
    "\n",
    "# Plot stationary gradient\n",
    "plt.plot(x, stationary_gradient, label=labels[3], color=colors[3], linestyle='dashed')\n",
    "\n",
    "# Plot negative gradients\n",
    "for i, y in enumerate(negative_gradients, start=4):\n",
    "    plt.plot(x, y, label=labels[i], color=colors[i])\n",
    "\n",
    "# Customize plot\n",
    "plt.axhline(0, color='gray', linewidth=0.5)\n",
    "plt.axvline(0, color='gray', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Plot of Positive, Stationary, and Negative Gradients\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.dates import DateFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage2severity(value):\n",
    "    return (\n",
    "        1 if 0 <= value < 5 else\n",
    "        2 if 5 <= value < 20 else\n",
    "        3 if 20 <= value < 40 else\n",
    "        4 if 40 <= value < 75 else\n",
    "        5 if 75 <= value <= 100 else\n",
    "        6\n",
    "    )\n",
    "    \n",
    "def calc_counterPercentage(threshold_percentages):\n",
    "    counter_feature = {}\n",
    "    for modex_idx, values_pred in threshold_percentages.items():\n",
    "        values_pred = dict(sorted(values_pred.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "        for name_feat, percentage in values_pred.items():\n",
    "            if name_feat in counter_feature:\n",
    "                counter_feature[name_feat][\"count\"] = counter_feature[name_feat][\"count\"] + 1\n",
    "                counter_feature[name_feat][\"percentage\"] = counter_feature[name_feat][\"percentage\"] + percentage\n",
    "            else:\n",
    "                counter_feature[name_feat] = {\"count\": 1, \"percentage\": percentage}\n",
    "\n",
    "    counter_feature_s1 = dict(sorted(counter_feature.items(), key=lambda item: item[1]['count'], reverse=True)[:10])\n",
    "    counter_feature_s2 = dict(sorted(counter_feature_s1.items(), key=lambda item: item[1]['percentage'] // len(model_array), reverse=True))\n",
    "    #counter_feature_s2_rank = dict(sorted(counter_feature_s1.items(), key=lambda item: item[1]['count'], reverse=True))\n",
    "\n",
    "    for key, value in counter_feature_s2.items():\n",
    "        counter_feature_s2[key]['count'] = (counter_feature_s2[key]['count'] / len(model_array)) * 100\n",
    "        counter_feature_s2[key]['severity'] = percentage2severity(counter_feature_s2[key]['percentage'] // len(model_array))\n",
    "        counter_feature_s2[key]['percentage'] = (counter_feature_s2[key]['percentage'] // len(model_array))\n",
    "\n",
    "    # Find Which Model Have Highest Confidence\n",
    "    counter_feature_plot = {}\n",
    "    for index, value in counter_feature_s2.items():\n",
    "        higher_data = {\"model\": 0, \"percentage\": 0}\n",
    "        for model_idx in threshold_percentages:\n",
    "            if index in threshold_percentages[model_idx]:\n",
    "                if higher_data[\"percentage\"] <= threshold_percentages[model_idx][index]:\n",
    "                    higher_data[\"model\"] = model_idx\n",
    "                    higher_data[\"percentage\"] = threshold_percentages[model_idx][index]\n",
    "        \n",
    "        counter_feature_plot[index] = higher_data['model']\n",
    "\n",
    "    return counter_feature_s2, counter_feature_plot\n",
    "\n",
    "def calc_counterPercentageTrending(threshold_percentages):\n",
    "    counter_feature = {}\n",
    "    for modex_idx, values_pred in threshold_percentages.items():\n",
    "        for name_feat, percentage in values_pred.items():\n",
    "            if name_feat in counter_feature:\n",
    "                if percentage > 5.0:\n",
    "                    counter_feature[name_feat][\"count\"] = counter_feature[name_feat][\"count\"] + 1\n",
    "                    counter_feature[name_feat][\"percentage\"] = counter_feature[name_feat][\"percentage\"] + percentage\n",
    "            else:\n",
    "                counter_feature[name_feat] = {\"count\": 1, \"percentage\": percentage}\n",
    "\n",
    "    for key, value in counter_feature.items():\n",
    "        counter_feature[key]['count'] = (counter_feature[key]['count'] / len(model_array)) * 100\n",
    "        if counter_feature[key]['count'] >= 20.0:\n",
    "            counter_feature[key]['severity'] = percentage2severity(counter_feature[key]['percentage'] // len(model_array))\n",
    "            counter_feature[key]['percentage'] = (counter_feature[key]['percentage'] // len(model_array))\n",
    "        else:\n",
    "            counter_feature[key]['severity'] = 1\n",
    "            counter_feature[key]['percentage'] = 0.0\n",
    "\n",
    "    return counter_feature\n",
    "\n",
    "def do_plotSeverityRank():\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    gs = GridSpec(4, 3, figure=fig)\n",
    "\n",
    "    feature_index_list = [feature_set.index(feat_name) for feat_name in list(counter_feature_s2.keys())]\n",
    "    for idx, (feature_index_now) in enumerate(feature_index_list[:4]):\n",
    "        model_idx_highest = counter_feature_plot[feature_set[feature_index_now]]\n",
    "\n",
    "        ax = fig.add_subplot(gs[idx, :2])\n",
    "        ax.plot(df_timestamp, temp_ypreds[model_idx_highest][:, feature_index_now], color='blue', label='Prediction')\n",
    "        ax.plot(df_timestamp, df_feature[:, feature_index_now], color='red', label='Original')\n",
    "        ax.set_title(feature_set[feature_index_now])\n",
    "        ax.legend() \n",
    "        ax.grid(True)\n",
    "\n",
    "    date_format = DateFormatter(\"%d/%m/%Y - %H:%M\")  # Define the desired format\n",
    "    plt.gca().xaxis.set_major_formatter(date_format)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    y2 = list(counter_feature_s2.keys())\n",
    "    x2 = [value['severity'] for value in counter_feature_s2.values()]\n",
    "    x2_c = [value['count'] for value in counter_feature_s2.values()]\n",
    "\n",
    "    norm_x2 = [(val - 1) / 5 for val in x2]\n",
    "    cmap = LinearSegmentedColormap.from_list('severity_colormap', ['green', 'yellow', 'red'])\n",
    "    colors = [cmap(norm) for norm in norm_x2]\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[:3, 2])\n",
    "    bars = ax3.barh(y2, x2, color=colors)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=1, vmax=6))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax3, orientation='vertical', label='Severity')\n",
    "\n",
    "    for bar, perc in zip(bars, x2_c):\n",
    "        width = bar.get_width()  # Get the width of the bar\n",
    "        ax3.text(\n",
    "            width - 0.1,             # X-coordinate (inside the bar, near the right edge)\n",
    "            bar.get_y() + bar.get_height() / 2,  # Y-coordinate (center of the bar)\n",
    "            f\"{int(perc)}%\",            # Text label (percentage with % sign)\n",
    "            va='center',           # Vertical alignment\n",
    "            ha='right',            # Horizontal alignment\n",
    "            color='black',         # Text color for visibility\n",
    "            fontsize=9            # Font size\n",
    "        )\n",
    "    ax3.invert_yaxis()\n",
    "    ax3.set_xticks(range(1, 8))\n",
    "    ax3.set_ylabel(\"Parameter\")\n",
    "    ax3.set_xlabel(\"Severity\")\n",
    "    ax3.set_title(\"Severity Rank\")\n",
    "\n",
    "    #fig.suptitle(f\"{df_anomaly_unplaned.values[failure_index_list, 4]}_{df_anomaly_unplaned.values[failure_index_list, 0]}\", fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def fetch_between_dates(start_date, end_date, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT * FROM {table_name} WHERE timestamp BETWEEN ? AND ?\n",
    "    \"\"\", (start_date, end_date))\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not rows:\n",
    "        return np.array([])\n",
    "    \n",
    "    return np.array(rows)\n",
    "\n",
    "def convert_timestamp(timestamp_str):\n",
    "    dt = datetime.fromisoformat(timestamp_str)\n",
    "    return pd.Timestamp(dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = ['Active Power', 'Reactive Power', 'Governor speed actual', 'UGB X displacement', 'UGB Y displacement',\n",
    "    'LGB X displacement', 'LGB Y displacement', 'TGB X displacement',\n",
    "    'TGB Y displacement', 'Stator winding temperature 13',\n",
    "    'Stator winding temperature 14', 'Stator winding temperature 15',\n",
    "    'Surface Air Cooler Air Outlet Temperature',\n",
    "    'Surface Air Cooler Water Inlet Temperature',\n",
    "    'Surface Air Cooler Water Outlet Temperature',\n",
    "    'Stator core temperature', 'UGB metal temperature',\n",
    "    'LGB metal temperature 1', 'LGB metal temperature 2',\n",
    "    'LGB oil temperature', 'Penstock Flow', 'Turbine flow',\n",
    "    'UGB cooling water flow', 'LGB cooling water flow',\n",
    "    'Generator cooling water flow', 'Governor Penstock Pressure',\n",
    "    'Penstock pressure', 'Opening Wicked Gate', 'UGB Oil Contaminant',\n",
    "    'Gen Thrust Bearing Oil Contaminant']\n",
    "\n",
    "# feature_set = ['Active Power', 'Governor speed actual', \n",
    "#     'UGB X displacement', 'UGB Y displacement', 'LGB X displacement', 'LGB Y displacement', 'TGB X displacement', 'TGB Y displacement', \n",
    "#     'Stator core temperature', 'Stator winding temperature 13', 'Stator winding temperature 14', 'Stator winding temperature 15',\n",
    "#     'Surface Air Cooler Air Outlet Temperature', 'Surface Air Cooler Water Inlet Temperature', 'Surface Air Cooler Water Outlet Temperature',\n",
    "#     'Gen Voltage Phase 1', 'Gen Voltage Phase 2', 'Gen Voltage Phase 3',\n",
    "#     'Gen Current Phase 1', 'Gen Current Phase 2', 'Gen Current Phase 3', \n",
    "#     'UGB metal temperature', 'LGB metal temperature 1', 'LGB metal temperature 2',\n",
    "#     'UGB oil temperature', 'LGB oil temperature', 'UGB cooling water flow', 'LGB cooling water flow', 'Generator cooling water flow',\n",
    "#     'UGB Oil Contaminant', 'Gen Thrust Bearing Oil Contaminant',\n",
    "#     'Penstock Flow', 'Turbine flow', 'Governor Penstock Pressure', 'Penstock pressure', 'Opening Wicked Gate']\n",
    "\n",
    "model_array = [\"Attention\", \"DTAAD\", \"MAD_GAN\", \"TranAD\", \"DAGMM\", \"USAD\", \"OmniAnomaly\"]\n",
    "\n",
    "# window_size = 15\n",
    "# kernel = np.ones(window_size) / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect(\"db_data/original_data.db\")\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute(f\"\"\"SELECT * FROM original_data order by rowid desc LIMIT 1\"\"\")\n",
    "# rows = cursor.fetchall()\n",
    "# conn.close()\n",
    "# last_date = np.datetime64(np.array(rows)[:, 1][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates_lastest = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S\") #\"2025-03-27T05:36:00\" \n",
    "timestamp = datetime.strptime(end_dates_lastest, \"%Y-%m-%dT%H:%M:%S\")\n",
    "hours_2before = timestamp - timedelta(hours=2)\n",
    "beofre_15min = timestamp - timedelta(minutes=60)\n",
    "hours_2before_str = hours_2before.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "threshold_percentages = {}\n",
    "for idx_model, (model_name) in enumerate(model_array):\n",
    "    now_fetched = fetch_between_dates(beofre_15min.strftime(\"%Y-%m-%dT%H:%M:%S\"), end_dates_lastest, \"db/threshold_data.db\", model_name)[-1, 2:]\n",
    "\n",
    "    threshold_pass = {}\n",
    "    for idx_sensor, sensor_thre in enumerate(now_fetched):\n",
    "        threshold_pass[feature_set[idx_sensor]] = float(sensor_thre)\n",
    "\n",
    "    threshold_percentages[idx_model] = threshold_pass\n",
    "\n",
    "temp_original_data = fetch_between_dates(hours_2before_str, end_dates_lastest, \"db/original_data.db\", \"original_data\")\n",
    "df_timestamp, df_feature = temp_original_data[:, 1], temp_original_data[:, 2:].astype(np.float16)\n",
    "df_timestamp = np.array([convert_timestamp(now_str) for now_str in df_timestamp])\n",
    "\n",
    "temp_ypreds = {}\n",
    "for idx_model, (model_name) in enumerate(model_array):\n",
    "    temp_ypreds[idx_model] = fetch_between_dates(hours_2before_str, end_dates_lastest, \"db/pred_data.db\", model_name)[:, 2:].astype(np.float16)\n",
    "\n",
    "counter_feature_s2, counter_feature_plot = calc_counterPercentage(threshold_percentages)\n",
    "df_feature_send = []\n",
    "y_pred_send = []\n",
    "\n",
    "feature_index_list = [feature_set.index(feat_name) for feat_name in list(counter_feature_s2.keys())]\n",
    "for idx, (feature_index_now) in enumerate(feature_index_list[:4]):\n",
    "    model_idx_highest = counter_feature_plot[feature_set[feature_index_now]]\n",
    "\n",
    "    df_feature_send.append(temp_ypreds[model_idx_highest][:, feature_index_now])\n",
    "    y_pred_send.append(df_feature[:, feature_index_now])\n",
    "\n",
    "df_feature_send = np.vstack(df_feature_send).T\n",
    "y_pred_send = np.vstack(y_pred_send).T\n",
    "\n",
    "# To Send counter_feature_s2, df_feature_send, y_pred_send\n",
    "\n",
    "fig = do_plotSeverityRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "l = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_send = []\n",
    "y_pred_send = []\n",
    "loss_send = []\n",
    "thr_now_model = []\n",
    "\n",
    "feature_index_list = [feature_set.index(feat_name) for feat_name in list(counter_feature_s2.keys())]\n",
    "for idx, (feature_index_now) in enumerate(feature_index_list[:4]):\n",
    "    model_idx_highest = counter_feature_plot[feature_set[feature_index_now]]\n",
    "\n",
    "    df_feature_send.append(temp_ypreds[model_idx_highest][:, feature_index_now])\n",
    "    y_pred_send.append(df_feature[:, feature_index_now])\n",
    "    \n",
    "    loss_send.append(df_feature[:, feature_index_now])\n",
    "    thr_now_model.append(float(model_thr[model_array[model_idx_highest]][feature_index_now]))\n",
    "\n",
    "\n",
    "# df_feature_send = np.vstack(df_feature_send).T\n",
    "# y_pred_send = np.vstack(y_pred_send).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ypreds[model_idx_highest][:, feature_index_now]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l(, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_send[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_now_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_feature = {}\n",
    "for modex_idx, values_pred in threshold_percentages.items():\n",
    "    values_pred = dict(sorted(values_pred.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "    for name_feat, percentage in values_pred.items():\n",
    "        if name_feat in counter_feature:\n",
    "            counter_feature[name_feat][\"count\"] = counter_feature[name_feat][\"count\"] + 1\n",
    "            counter_feature[name_feat][\"percentage\"] = counter_feature[name_feat][\"percentage\"] + percentage\n",
    "        else:\n",
    "            counter_feature[name_feat] = {\"count\": 1, \"percentage\": percentage}\n",
    "\n",
    "counter_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_feature_s1 = dict(sorted(counter_feature.items(), key=lambda item: item[1]['count'], reverse=True)[:10])\n",
    "counter_feature_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates_lastest = \"2025-03-27T05:36:00\" \n",
    "timestamp = datetime.strptime(end_dates_lastest, \"%Y-%m-%dT%H:%M:%S\")\n",
    "hours_2before = timestamp - timedelta(days=30)\n",
    "hours_2before_str = hours_2before.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "data_between_dates = fetch_between_dates(hours_2before_str, end_dates_lastest, \"db/severity_trendings.db\", \"severity_trendings\")\n",
    "\n",
    "data_feature = data_between_dates[:, 2:].astype(float)\n",
    "data_timestamp = np.array([convert_timestamp(now_str) for now_str in data_between_dates[:, 1]])\n",
    "\n",
    "fig, axes = plt.subplots(30, 1, figsize=(10, 75))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(data_timestamp, scipy.signal.savgol_filter(data_feature[:, i], 50, 3))  # Plot data for each row\n",
    "    ax.set_title(f'{feature_set[i]} ', fontsize=10)  # Set title\n",
    "    #ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_ylabel(\"Severity Percentage\")\n",
    "    #ax.set_xticks(data_timestamp[::7])  # Reduce number of ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denormalize3(a_norm, min_a, max_a):\n",
    "#     return a_norm * (max_a - min_a + 0.0001) + min_a\n",
    "\n",
    "# with open('normalize_2023.pickle', 'rb') as handle:\n",
    "#     normalize_obj = pickle.load(handle)\n",
    "#     min_a, max_a = normalize_obj['min_a'], normalize_obj['max_a']\n",
    "\n",
    "# with open('model_thr.pickle', 'rb') as handle:\n",
    "#     model_thr = pickle.load(handle)\n",
    "\n",
    "# for model_now in model_array:\n",
    "#     model_thr[model_now] = denormalize3(np.array(model_thr[model_now]), min_a, max_a).tolist()\n",
    "\n",
    "# with open('model_thr.pickle', 'wb') as handle:\n",
    "#     pickle.dump(model_thr, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
