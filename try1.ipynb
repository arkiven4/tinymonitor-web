{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 87840 rows into 'additional_original_data'\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(\"db/original_data.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the table if it doesn't already exist\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS additional_original_data (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    timestamp TEXT,\n",
    "    Grid_Selection INTEGER\n",
    ")\n",
    "''')\n",
    "\n",
    "# Define the start and end datetime\n",
    "start_time = datetime(2024, 1, 1, 0, 0, 0)\n",
    "end_time = datetime(2024, 12, 31, 23, 59, 59)\n",
    "interval = timedelta(minutes=6)\n",
    "\n",
    "# Generate data\n",
    "data_to_insert = []\n",
    "current_time = start_time\n",
    "while current_time <= end_time:\n",
    "    timestamp = current_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    grid_value = 0 if random.random() < 0.8 else 1  # 80% 0, 20% 1\n",
    "    data_to_insert.append((timestamp, grid_value))\n",
    "    current_time += interval\n",
    "\n",
    "# Insert into the database\n",
    "cursor.executemany('''\n",
    "INSERT INTO additional_original_data (timestamp, Grid_Selection)\n",
    "VALUES (?, ?)\n",
    "''', data_to_insert)\n",
    "\n",
    "# Commit and close\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Inserted {len(data_to_insert)} rows into 'additional_original_data'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(\"/home/arkiven4/Downloads/Sample_Down Time Events.xlsx\")\n",
    "df['Start'] = pd.to_datetime(df['Start Date'] + ' ' + df['Start Time'])\n",
    "df['End'] = pd.to_datetime(df['End Date'] + ' ' + df['End Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start Date</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Date</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Event</th>\n",
       "      <th>Category</th>\n",
       "      <th>Plant</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>08:25:00</td>\n",
       "      <td>2023-02-19</td>\n",
       "      <td>10:16:00</td>\n",
       "      <td>LGS#1 Trip by over frequency (Total blackout)</td>\n",
       "      <td>PV</td>\n",
       "      <td>LGS1</td>\n",
       "      <td>2023-02-19 08:25:00</td>\n",
       "      <td>2023-02-19 10:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>20:15:00</td>\n",
       "      <td>2023-02-27</td>\n",
       "      <td>20:35:00</td>\n",
       "      <td>LGS#1 Trip by Line#1 and Line#2 Trip</td>\n",
       "      <td>PV</td>\n",
       "      <td>LGS1</td>\n",
       "      <td>2023-02-27 20:15:00</td>\n",
       "      <td>2023-02-27 20:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>22:53:00</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>23:01:00</td>\n",
       "      <td>LGS#1 Trip to standstill</td>\n",
       "      <td>CD</td>\n",
       "      <td>LGS2</td>\n",
       "      <td>2023-03-10 22:53:00</td>\n",
       "      <td>2023-03-10 23:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>22:53:00</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>23:01:00</td>\n",
       "      <td>LGS#1 Trip to standstill</td>\n",
       "      <td>CD</td>\n",
       "      <td>LGS2</td>\n",
       "      <td>2023-03-10 22:53:00</td>\n",
       "      <td>2023-03-10 23:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>03:57:00</td>\n",
       "      <td>2023-04-24</td>\n",
       "      <td>04:32:00</td>\n",
       "      <td>LGS#1 trip by 186 TX hand reset lockout relay</td>\n",
       "      <td>CR</td>\n",
       "      <td>LGS3</td>\n",
       "      <td>2023-04-24 03:57:00</td>\n",
       "      <td>2023-04-24 04:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>20:13:00</td>\n",
       "      <td>2023-05-22</td>\n",
       "      <td>20:32:00</td>\n",
       "      <td>LGS#1 Trip by 86N energized</td>\n",
       "      <td>CR</td>\n",
       "      <td>LGS3</td>\n",
       "      <td>2023-05-22 20:13:00</td>\n",
       "      <td>2023-05-22 20:32:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>10:49:00</td>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>10:59:00</td>\n",
       "      <td>LGS#1 Trip by under frequency</td>\n",
       "      <td>PV</td>\n",
       "      <td>KGS1</td>\n",
       "      <td>2023-07-31 10:49:00</td>\n",
       "      <td>2023-07-31 10:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-09-16</td>\n",
       "      <td>08:16:00</td>\n",
       "      <td>2023-09-16</td>\n",
       "      <td>09:36:00</td>\n",
       "      <td>LGS#1 Trip by over frequency (Total blackout)</td>\n",
       "      <td>PV</td>\n",
       "      <td>KGS2</td>\n",
       "      <td>2023-09-16 08:16:00</td>\n",
       "      <td>2023-09-16 09:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>22:01:00</td>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>22:18:00</td>\n",
       "      <td>Power Hydro to FCE’s grid Blackout.</td>\n",
       "      <td>CD</td>\n",
       "      <td>BGS1</td>\n",
       "      <td>2023-10-04 22:01:00</td>\n",
       "      <td>2023-10-04 22:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>10:49:00</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>11:18:00</td>\n",
       "      <td>Furnace grid blackout</td>\n",
       "      <td>CR</td>\n",
       "      <td>BGS2</td>\n",
       "      <td>2022-03-31 10:49:00</td>\n",
       "      <td>2022-03-31 11:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>10:07:00</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>13:50:00</td>\n",
       "      <td>LGS#1 Trip by 86N energized</td>\n",
       "      <td>CD</td>\n",
       "      <td>LGS1</td>\n",
       "      <td>2022-03-26 10:07:00</td>\n",
       "      <td>2022-03-26 13:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>16:17:00</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>22:28:00</td>\n",
       "      <td>LGS#1 suddenly trip by 86N due to Rotor shaft ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>LGS1</td>\n",
       "      <td>2022-03-26 16:17:00</td>\n",
       "      <td>2022-03-26 22:28:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-08-27</td>\n",
       "      <td>13:50:00</td>\n",
       "      <td>2022-08-27</td>\n",
       "      <td>14:51:00</td>\n",
       "      <td>Total blackout</td>\n",
       "      <td>CD</td>\n",
       "      <td>LGS2</td>\n",
       "      <td>2022-08-27 13:50:00</td>\n",
       "      <td>2022-08-27 14:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-09-16</td>\n",
       "      <td>08:16:00</td>\n",
       "      <td>2023-09-16</td>\n",
       "      <td>09:36:00</td>\n",
       "      <td>LGS#1 Trip by over frequency (Total blackout)</td>\n",
       "      <td>PV</td>\n",
       "      <td>LGS2</td>\n",
       "      <td>2023-09-16 08:16:00</td>\n",
       "      <td>2023-09-16 09:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>22:01:00</td>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>22:18:00</td>\n",
       "      <td>Power Hydro to FCE’s grid Blackout.</td>\n",
       "      <td>CD</td>\n",
       "      <td>LGS3</td>\n",
       "      <td>2023-10-04 22:01:00</td>\n",
       "      <td>2023-10-04 22:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>10:49:00</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>11:18:00</td>\n",
       "      <td>Furnace grid blackout</td>\n",
       "      <td>CR</td>\n",
       "      <td>LGS3</td>\n",
       "      <td>2022-03-31 10:49:00</td>\n",
       "      <td>2022-03-31 11:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>10:07:00</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>13:50:00</td>\n",
       "      <td>LGS#1 Trip by 86N energized</td>\n",
       "      <td>CD</td>\n",
       "      <td>KGS1</td>\n",
       "      <td>2022-03-26 10:07:00</td>\n",
       "      <td>2022-03-26 13:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>16:17:00</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>22:28:00</td>\n",
       "      <td>LGS#1 suddenly trip by 86N due to Rotor shaft ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>KGS2</td>\n",
       "      <td>2022-03-26 16:17:00</td>\n",
       "      <td>2022-03-26 22:28:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-08-27</td>\n",
       "      <td>13:50:00</td>\n",
       "      <td>2022-08-27</td>\n",
       "      <td>14:51:00</td>\n",
       "      <td>Total blackout</td>\n",
       "      <td>CD</td>\n",
       "      <td>BGS1</td>\n",
       "      <td>2022-08-27 13:50:00</td>\n",
       "      <td>2022-08-27 14:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-09-16</td>\n",
       "      <td>08:16:00</td>\n",
       "      <td>2023-09-16</td>\n",
       "      <td>09:36:00</td>\n",
       "      <td>LGS#1 Trip by over frequency (Total blackout)</td>\n",
       "      <td>PV</td>\n",
       "      <td>BGS2</td>\n",
       "      <td>2023-09-16 08:16:00</td>\n",
       "      <td>2023-09-16 09:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>22:01:00</td>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>22:18:00</td>\n",
       "      <td>Power Hydro to FCE’s grid Blackout.</td>\n",
       "      <td>CD</td>\n",
       "      <td>LGS1</td>\n",
       "      <td>2023-10-04 22:01:00</td>\n",
       "      <td>2023-10-04 22:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>10:49:00</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>11:18:00</td>\n",
       "      <td>Furnace grid blackout</td>\n",
       "      <td>CR</td>\n",
       "      <td>LGS1</td>\n",
       "      <td>2022-03-31 10:49:00</td>\n",
       "      <td>2022-03-31 11:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>10:07:00</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>13:50:00</td>\n",
       "      <td>LGS#1 Trip by 86N energized</td>\n",
       "      <td>CD</td>\n",
       "      <td>LGS2</td>\n",
       "      <td>2022-03-26 10:07:00</td>\n",
       "      <td>2022-03-26 13:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>16:17:00</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>22:28:00</td>\n",
       "      <td>LGS#1 suddenly trip by 86N due to Rotor shaft ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>LGS2</td>\n",
       "      <td>2022-03-26 16:17:00</td>\n",
       "      <td>2022-03-26 22:28:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2022-08-27</td>\n",
       "      <td>13:50:00</td>\n",
       "      <td>2022-08-27</td>\n",
       "      <td>14:51:00</td>\n",
       "      <td>Total blackout</td>\n",
       "      <td>CD</td>\n",
       "      <td>LGS3</td>\n",
       "      <td>2022-08-27 13:50:00</td>\n",
       "      <td>2022-08-27 14:51:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-09-16</td>\n",
       "      <td>08:16:00</td>\n",
       "      <td>2023-09-16</td>\n",
       "      <td>09:36:00</td>\n",
       "      <td>LGS#1 Trip by over frequency (Total blackout)</td>\n",
       "      <td>PV</td>\n",
       "      <td>LGS3</td>\n",
       "      <td>2023-09-16 08:16:00</td>\n",
       "      <td>2023-09-16 09:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>22:01:00</td>\n",
       "      <td>2023-10-04</td>\n",
       "      <td>22:18:00</td>\n",
       "      <td>Power Hydro to FCE’s grid Blackout.</td>\n",
       "      <td>CD</td>\n",
       "      <td>KGS1</td>\n",
       "      <td>2023-10-04 22:01:00</td>\n",
       "      <td>2023-10-04 22:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>10:49:00</td>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>11:18:00</td>\n",
       "      <td>Furnace grid blackout</td>\n",
       "      <td>CR</td>\n",
       "      <td>KGS2</td>\n",
       "      <td>2022-03-31 10:49:00</td>\n",
       "      <td>2022-03-31 11:18:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>10:07:00</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>13:50:00</td>\n",
       "      <td>LGS#1 Trip by 86N energized</td>\n",
       "      <td>CD</td>\n",
       "      <td>BGS1</td>\n",
       "      <td>2022-03-26 10:07:00</td>\n",
       "      <td>2022-03-26 13:50:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>16:17:00</td>\n",
       "      <td>2022-03-26</td>\n",
       "      <td>22:28:00</td>\n",
       "      <td>LGS#1 suddenly trip by 86N due to Rotor shaft ...</td>\n",
       "      <td>PV</td>\n",
       "      <td>BGS2</td>\n",
       "      <td>2022-03-26 16:17:00</td>\n",
       "      <td>2022-03-26 22:28:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Start Date Start Time    End Date  End Time  \\\n",
       "0   2023-02-19   08:25:00  2023-02-19  10:16:00   \n",
       "1   2023-02-27   20:15:00  2023-02-27  20:35:00   \n",
       "2   2023-03-10   22:53:00  2023-03-10  23:01:00   \n",
       "3   2023-03-10   22:53:00  2023-03-10  23:01:00   \n",
       "4   2023-04-24   03:57:00  2023-04-24  04:32:00   \n",
       "5   2023-05-22   20:13:00  2023-05-22  20:32:00   \n",
       "6   2023-07-31   10:49:00  2023-07-31  10:59:00   \n",
       "7   2023-09-16   08:16:00  2023-09-16  09:36:00   \n",
       "8   2023-10-04   22:01:00  2023-10-04  22:18:00   \n",
       "9   2022-03-31   10:49:00  2022-03-31  11:18:00   \n",
       "10  2022-03-26   10:07:00  2022-03-26  13:50:00   \n",
       "11  2022-03-26   16:17:00  2022-03-26  22:28:00   \n",
       "12  2022-08-27   13:50:00  2022-08-27  14:51:00   \n",
       "13  2023-09-16   08:16:00  2023-09-16  09:36:00   \n",
       "14  2023-10-04   22:01:00  2023-10-04  22:18:00   \n",
       "15  2022-03-31   10:49:00  2022-03-31  11:18:00   \n",
       "16  2022-03-26   10:07:00  2022-03-26  13:50:00   \n",
       "17  2022-03-26   16:17:00  2022-03-26  22:28:00   \n",
       "18  2022-08-27   13:50:00  2022-08-27  14:51:00   \n",
       "19  2023-09-16   08:16:00  2023-09-16  09:36:00   \n",
       "20  2023-10-04   22:01:00  2023-10-04  22:18:00   \n",
       "21  2022-03-31   10:49:00  2022-03-31  11:18:00   \n",
       "22  2022-03-26   10:07:00  2022-03-26  13:50:00   \n",
       "23  2022-03-26   16:17:00  2022-03-26  22:28:00   \n",
       "24  2022-08-27   13:50:00  2022-08-27  14:51:00   \n",
       "25  2023-09-16   08:16:00  2023-09-16  09:36:00   \n",
       "26  2023-10-04   22:01:00  2023-10-04  22:18:00   \n",
       "27  2022-03-31   10:49:00  2022-03-31  11:18:00   \n",
       "28  2022-03-26   10:07:00  2022-03-26  13:50:00   \n",
       "29  2022-03-26   16:17:00  2022-03-26  22:28:00   \n",
       "\n",
       "                                                Event Category Plant  \\\n",
       "0       LGS#1 Trip by over frequency (Total blackout)       PV  LGS1   \n",
       "1                LGS#1 Trip by Line#1 and Line#2 Trip       PV  LGS1   \n",
       "2                            LGS#1 Trip to standstill       CD  LGS2   \n",
       "3                            LGS#1 Trip to standstill       CD  LGS2   \n",
       "4       LGS#1 trip by 186 TX hand reset lockout relay       CR  LGS3   \n",
       "5                         LGS#1 Trip by 86N energized       CR  LGS3   \n",
       "6                       LGS#1 Trip by under frequency       PV  KGS1   \n",
       "7       LGS#1 Trip by over frequency (Total blackout)       PV  KGS2   \n",
       "8                 Power Hydro to FCE’s grid Blackout.       CD  BGS1   \n",
       "9                               Furnace grid blackout       CR  BGS2   \n",
       "10                        LGS#1 Trip by 86N energized       CD  LGS1   \n",
       "11  LGS#1 suddenly trip by 86N due to Rotor shaft ...       PV  LGS1   \n",
       "12                                     Total blackout       CD  LGS2   \n",
       "13      LGS#1 Trip by over frequency (Total blackout)       PV  LGS2   \n",
       "14                Power Hydro to FCE’s grid Blackout.       CD  LGS3   \n",
       "15                              Furnace grid blackout       CR  LGS3   \n",
       "16                        LGS#1 Trip by 86N energized       CD  KGS1   \n",
       "17  LGS#1 suddenly trip by 86N due to Rotor shaft ...       PV  KGS2   \n",
       "18                                     Total blackout       CD  BGS1   \n",
       "19      LGS#1 Trip by over frequency (Total blackout)       PV  BGS2   \n",
       "20                Power Hydro to FCE’s grid Blackout.       CD  LGS1   \n",
       "21                              Furnace grid blackout       CR  LGS1   \n",
       "22                        LGS#1 Trip by 86N energized       CD  LGS2   \n",
       "23  LGS#1 suddenly trip by 86N due to Rotor shaft ...       PV  LGS2   \n",
       "24                                     Total blackout       CD  LGS3   \n",
       "25      LGS#1 Trip by over frequency (Total blackout)       PV  LGS3   \n",
       "26                Power Hydro to FCE’s grid Blackout.       CD  KGS1   \n",
       "27                              Furnace grid blackout       CR  KGS2   \n",
       "28                        LGS#1 Trip by 86N energized       CD  BGS1   \n",
       "29  LGS#1 suddenly trip by 86N due to Rotor shaft ...       PV  BGS2   \n",
       "\n",
       "                 Start                 End  \n",
       "0  2023-02-19 08:25:00 2023-02-19 10:16:00  \n",
       "1  2023-02-27 20:15:00 2023-02-27 20:35:00  \n",
       "2  2023-03-10 22:53:00 2023-03-10 23:01:00  \n",
       "3  2023-03-10 22:53:00 2023-03-10 23:01:00  \n",
       "4  2023-04-24 03:57:00 2023-04-24 04:32:00  \n",
       "5  2023-05-22 20:13:00 2023-05-22 20:32:00  \n",
       "6  2023-07-31 10:49:00 2023-07-31 10:59:00  \n",
       "7  2023-09-16 08:16:00 2023-09-16 09:36:00  \n",
       "8  2023-10-04 22:01:00 2023-10-04 22:18:00  \n",
       "9  2022-03-31 10:49:00 2022-03-31 11:18:00  \n",
       "10 2022-03-26 10:07:00 2022-03-26 13:50:00  \n",
       "11 2022-03-26 16:17:00 2022-03-26 22:28:00  \n",
       "12 2022-08-27 13:50:00 2022-08-27 14:51:00  \n",
       "13 2023-09-16 08:16:00 2023-09-16 09:36:00  \n",
       "14 2023-10-04 22:01:00 2023-10-04 22:18:00  \n",
       "15 2022-03-31 10:49:00 2022-03-31 11:18:00  \n",
       "16 2022-03-26 10:07:00 2022-03-26 13:50:00  \n",
       "17 2022-03-26 16:17:00 2022-03-26 22:28:00  \n",
       "18 2022-08-27 13:50:00 2022-08-27 14:51:00  \n",
       "19 2023-09-16 08:16:00 2023-09-16 09:36:00  \n",
       "20 2023-10-04 22:01:00 2023-10-04 22:18:00  \n",
       "21 2022-03-31 10:49:00 2022-03-31 11:18:00  \n",
       "22 2022-03-26 10:07:00 2022-03-26 13:50:00  \n",
       "23 2022-03-26 16:17:00 2022-03-26 22:28:00  \n",
       "24 2022-08-27 13:50:00 2022-08-27 14:51:00  \n",
       "25 2023-09-16 08:16:00 2023-09-16 09:36:00  \n",
       "26 2023-10-04 22:01:00 2023-10-04 22:18:00  \n",
       "27 2022-03-31 10:49:00 2022-03-31 11:18:00  \n",
       "28 2022-03-26 10:07:00 2022-03-26 13:50:00  \n",
       "29 2022-03-26 16:17:00 2022-03-26 22:28:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Group and reshape by Plant and Category\n",
    "category_counts = df.groupby(['Plant', 'Category']).size().unstack(fill_value=0)\n",
    "\n",
    "# Prepare final data\n",
    "final_data = {\n",
    "    'plants': list(category_counts.index),\n",
    "    'data': [\n",
    "        {\n",
    "            'label': col,\n",
    "            'data': category_counts[col].tolist(),\n",
    "        }\n",
    "        for col in category_counts.columns\n",
    "    ],\n",
    "    'raw_events': df[['Start Date', 'Plant', 'Category']].to_dict(orient='records')  # for filtering later\n",
    "}\n",
    "\n",
    "# # Save to pickle\n",
    "# with open(settings.MONITORINGDB_PATH + 'db/number_of_event.pickle', 'wb') as handle:\n",
    "#     pickle.dump(final_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'plants': ['BGS1', 'BGS2', 'KGS1', 'KGS2', 'LGS1', 'LGS2', 'LGS3'],\n",
       " 'data': [{'label': 'CD', 'data': [3, 0, 2, 0, 2, 4, 2]},\n",
       "  {'label': 'CR', 'data': [0, 1, 0, 1, 1, 0, 3]},\n",
       "  {'label': 'PV', 'data': [0, 2, 1, 2, 3, 2, 1]}],\n",
       " 'raw_events': [{'Start Date': Timestamp('2023-02-19 00:00:00'),\n",
       "   'Plant': 'LGS1',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2023-02-27 00:00:00'),\n",
       "   'Plant': 'LGS1',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2023-03-10 00:00:00'),\n",
       "   'Plant': 'LGS2',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2023-03-10 00:00:00'),\n",
       "   'Plant': 'LGS2',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2023-04-24 00:00:00'),\n",
       "   'Plant': 'LGS3',\n",
       "   'Category': 'CR'},\n",
       "  {'Start Date': Timestamp('2023-05-22 00:00:00'),\n",
       "   'Plant': 'LGS3',\n",
       "   'Category': 'CR'},\n",
       "  {'Start Date': Timestamp('2023-07-31 00:00:00'),\n",
       "   'Plant': 'KGS1',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2023-09-16 00:00:00'),\n",
       "   'Plant': 'KGS2',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2023-10-04 00:00:00'),\n",
       "   'Plant': 'BGS1',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2022-03-31 00:00:00'),\n",
       "   'Plant': 'BGS2',\n",
       "   'Category': 'CR'},\n",
       "  {'Start Date': Timestamp('2022-03-26 00:00:00'),\n",
       "   'Plant': 'LGS1',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2022-03-26 00:00:00'),\n",
       "   'Plant': 'LGS1',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2022-08-27 00:00:00'),\n",
       "   'Plant': 'LGS2',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2023-09-16 00:00:00'),\n",
       "   'Plant': 'LGS2',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2023-10-04 00:00:00'),\n",
       "   'Plant': 'LGS3',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2022-03-31 00:00:00'),\n",
       "   'Plant': 'LGS3',\n",
       "   'Category': 'CR'},\n",
       "  {'Start Date': Timestamp('2022-03-26 00:00:00'),\n",
       "   'Plant': 'KGS1',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2022-03-26 00:00:00'),\n",
       "   'Plant': 'KGS2',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2022-08-27 00:00:00'),\n",
       "   'Plant': 'BGS1',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2023-09-16 00:00:00'),\n",
       "   'Plant': 'BGS2',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2023-10-04 00:00:00'),\n",
       "   'Plant': 'LGS1',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2022-03-31 00:00:00'),\n",
       "   'Plant': 'LGS1',\n",
       "   'Category': 'CR'},\n",
       "  {'Start Date': Timestamp('2022-03-26 00:00:00'),\n",
       "   'Plant': 'LGS2',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2022-03-26 00:00:00'),\n",
       "   'Plant': 'LGS2',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2022-08-27 00:00:00'),\n",
       "   'Plant': 'LGS3',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2023-09-16 00:00:00'),\n",
       "   'Plant': 'LGS3',\n",
       "   'Category': 'PV'},\n",
       "  {'Start Date': Timestamp('2023-10-04 00:00:00'),\n",
       "   'Plant': 'KGS1',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2022-03-31 00:00:00'),\n",
       "   'Plant': 'KGS2',\n",
       "   'Category': 'CR'},\n",
       "  {'Start Date': Timestamp('2022-03-26 00:00:00'),\n",
       "   'Plant': 'BGS1',\n",
       "   'Category': 'CD'},\n",
       "  {'Start Date': Timestamp('2022-03-26 00:00:00'),\n",
       "   'Plant': 'BGS2',\n",
       "   'Category': 'PV'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tokenizers import PreTokenizedString\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.stats import spearmanr\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "from django.conf import settings\n",
    "import apis.commons as commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tag_mapping = {\n",
    "    'LGS1 Active Power': 'U-LGS1-Active-Power-AI',\n",
    "    'LGS1-Auxiliary Grid (0 = ACTIVE)': 'U-LGS1-N75-15-0-AI',\n",
    "    'LGS1 Governor Unit Speed Actual': 'U-LGS1-SI-81101-AI',\n",
    "    \n",
    "    'LGS2 Active Power': 'U-LGS2-Active-Power-AI',\n",
    "    'LGS2-Auxiliary Grid (0 = ACTIVE)': 'U-LGS2-N75-25-0-AI',\n",
    "    'LGS2 Governor Unit Speed Actual': 'U-LGS2-SI-81201-AI',\n",
    "    \n",
    "    'LGS3 Active Power': 'U-LGS3_Active-Power-AI',\n",
    "    'LGS3-Auxiliary Grid (0 = ACTIVE)': 'U-LGS3-N75-35-0-AI',\n",
    "    'LGS3 Governor Unit Speed Actual': 'U-LGS3_SI_81301_I_Eng-AI',\n",
    "\n",
    "    'Avg Hydro Power Available 1D (Avg)': 'U-PWR-HYDRO-AI-AVGD',\n",
    "    'Total Hydro Power Daily (Tot)': 'U-HGST-Power-AI-DTT',\n",
    "    'Total Larona Power Daily (Tot)': 'U-PWR-LAR-TOT-DTT',\n",
    "    'Total Balambano Power Daily (Tot)': 'U-PWR-BAL-TOT-DTT',\n",
    "    'Total Karebbe Power Daily (Tot)': 'U-PWR-KAR-TOT-DTT', \n",
    "\n",
    "    # BGS\n",
    "    'BGS1 Power': 'U-BGS1-Power-AI',\n",
    "    'BGS1-Auxiliary Grid (0 = ACTIVE)': 'U-BGS1-N75-45-0-AI',\n",
    "    'GEN SPEED BGS1': 'U-BGS1_I_T_SPEED-AI',\n",
    "\n",
    "    'BGS2 Power': 'U-BGS2-Power-AI',\n",
    "    'BGS2-Auxiliary Grid (0 = ACTIVE)': 'U-BGS2-N75-55-0-AI',\n",
    "    'GEN SPEED BGS2': 'U-BGS2_I_T_SPEED-AI',\n",
    "\n",
    "    # KGS\n",
    "    'K U1 Active Power (MW)': 'U-KGS1-Active_Power_AI',\n",
    "    'KGS1-Auxiliary Grid (0 = ACTIVE)': 'U-KGS1-N75-65-0-AI',\n",
    "    'K U1 Turb Gov Turbine Speed (RPM)': 'U-KGS1-Turb_Gov_Turb_Speed-AI',\n",
    "\n",
    "    'K U2 Active Power (MW)': 'U-KGS2-Active_Power_AI',\n",
    "    'KGS2-Auxiliary Grid (0 = ACTIVE)': 'U-KGS2-N75-75-0-AI',\n",
    "    'K U2 Turb Gov Turbine Speed (RPM)': 'U-KGS2-Turb_Gov_Turb_Speed-AI',\n",
    "}\n",
    "\n",
    "reverse_mapping = {v: k for k, v in feature_tag_mapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U-LGS1-Active-Power-AI',\n",
       " 'U-LGS1-N75-15-0-AI',\n",
       " 'U-LGS1-SI-81101-AI',\n",
       " 'U-LGS2-Active-Power-AI',\n",
       " 'U-LGS2-N75-25-0-AI',\n",
       " 'U-LGS2-SI-81201-AI',\n",
       " 'U-LGS3_Active-Power-AI',\n",
       " 'U-LGS3-N75-35-0-AI',\n",
       " 'U-LGS3_SI_81301_I_Eng-AI',\n",
       " 'U-PWR-HYDRO-AI-AVGD',\n",
       " 'U-HGST-Power-AI-DTT',\n",
       " 'U-PWR-LAR-TOT-DTT',\n",
       " 'U-PWR-BAL-TOT-DTT',\n",
       " 'U-PWR-KAR-TOT-DTT',\n",
       " 'U-BGS1-Power-AI',\n",
       " 'U-BGS1-N75-45-0-AI',\n",
       " 'U-BGS1_I_T_SPEED-AI',\n",
       " 'U-BGS2-Power-AI',\n",
       " 'U-BGS2-N75-55-0-AI',\n",
       " 'U-BGS2_I_T_SPEED-AI',\n",
       " 'U-KGS1-Active_Power_AI',\n",
       " 'U-KGS1-N75-65-0-AI',\n",
       " 'U-KGS1-Turb_Gov_Turb_Speed-AI',\n",
       " 'U-KGS2-Active_Power_AI',\n",
       " 'U-KGS2-N75-75-0-AI',\n",
       " 'U-KGS2-Turb_Gov_Turb_Speed-AI']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v for k,v in feature_tag_mapping.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n",
      "/tmp/ipykernel_74857/118406144.py:12: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  value_resp = pd.read_csv(filepath)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "master_pd = \"\"\n",
    "column_name = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(\"data_csv/2024\"):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        tag_name = filepath.split(\"/\")[-1].split(\".\")[0]\n",
    "        feature_key = reverse_mapping.get(tag_name)\n",
    "        column_name.append(feature_key)\n",
    "\n",
    "        value_resp = pd.read_csv(filepath)\n",
    "        if count == 0:\n",
    "            value_resp['Timestamps'] = pd.to_datetime(value_resp['Timestamps'])\n",
    "            master_pd = value_resp\n",
    "        else:\n",
    "            master_pd = pd.concat([master_pd, value_resp['Values']], axis=1, join='inner')\n",
    "\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74857/1907765593.py:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  master_pd.replace('I/O Timeout', np.nan, inplace=True)\n",
      "/tmp/ipykernel_74857/1907765593.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  master_pd.replace('No Data', np.nan, inplace=True)\n",
      "/tmp/ipykernel_74857/1907765593.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  master_pd.replace('Open', np.nan, inplace=True)\n",
      "/tmp/ipykernel_74857/1907765593.py:16: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  master_pd = master_pd.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "master_pd = master_pd.values\n",
    "master_pd = pd.DataFrame(data=master_pd, columns=['TimeStamp'] + list(column_name)) #+ feature_set + ['Grid Selection'])\n",
    "master_pd = master_pd.reset_index(drop=True)\n",
    "master_pd.replace('I/O Timeout', np.nan, inplace=True)\n",
    "master_pd.replace('No Data', np.nan, inplace=True)\n",
    "master_pd.replace('Future Data Unsupported', np.nan, inplace=True)\n",
    "master_pd.replace('Closed', np.nan, inplace=True)\n",
    "master_pd.replace('Open', np.nan, inplace=True)\n",
    "\n",
    "for column_name in master_pd.columns:\n",
    "    if column_name != 'Load_Type' and column_name != 'TimeStamp':\n",
    "        master_pd[column_name] = pd.to_numeric(master_pd[column_name], downcast='float')\n",
    "\n",
    "master_pd = master_pd.sort_values(by='TimeStamp')\n",
    "master_pd = master_pd.reset_index(drop=True)\n",
    "master_pd = master_pd.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_pd.drop(['Unit breaker BGS2', 'Unit breaker BGS1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_pd.to_csv(\"kpi2024.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_pd = pd.read_csv(\"kpi2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_db_timeconst(feature_set, db_name=\"masters_data.db\", table_name=\"severity_trending\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table if it does not exist\n",
    "    columns = \", \".join([feature_name.replace(\" \", \"_\") for feature_name in feature_set])\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            timestamp TEXT,\n",
    "            {columns}\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def timeseries_savedb(df_timestamp, data, feature_set, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    #if len(data) == 30:\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    # Generate timestamp\n",
    "    timestamp = df_timestamp.isoformat()\n",
    "    \n",
    "    # Build column names for features, replacing spaces with underscores\n",
    "    feature_columns = ', '.join([feature_name.replace(\" \", \"_\") for feature_name in feature_set])\n",
    "    placeholders = ', '.join(['?' for _ in range(len(feature_set))])\n",
    "    \n",
    "    # Upsert using INSERT OR REPLACE\n",
    "    # Note: Your table must have a UNIQUE constraint on the timestamp column.\n",
    "    sql = f\"\"\"\n",
    "        INSERT OR REPLACE INTO {table_name} (timestamp, {feature_columns})\n",
    "        VALUES (?, {placeholders})\n",
    "    \"\"\"\n",
    "    cursor.execute(sql, (timestamp, *data))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def is_number(x):\n",
    "    return isinstance(x, (int, float))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plant_metadata = {\n",
    "    'Larona': [{\n",
    "        'name': \"LGS1\",\n",
    "        'active_power': 'LGS1 Active Power',\n",
    "        'rpm': 'LGS1 Governor Unit Speed Actual',\n",
    "        'aux': 'LGS1-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [20.944, 11.398]\n",
    "    },\n",
    "    {\n",
    "        'name': \"LGS2\",\n",
    "        'active_power': 'LGS2 Active Power',\n",
    "        'rpm': 'LGS2 Governor Unit Speed Actual',\n",
    "        'aux': 'LGS2-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [21.162, 8.49]\n",
    "    },\n",
    "    {\n",
    "        'name': \"LGS3\",\n",
    "        'active_power': 'LGS3 Active Power',\n",
    "        'rpm': 'LGS3 Governor Unit Speed Actual',\n",
    "        'aux': 'LGS3-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [19.66, 13.676]\n",
    "    }],\n",
    "    'Balambano': [{\n",
    "        'name': \"BGS1\",\n",
    "        'active_power': 'BGS1 Power',\n",
    "        'rpm': 'GEN SPEED BGS1',\n",
    "        'aux': 'BGS1-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [20.944, 11.398]\n",
    "    },\n",
    "    {\n",
    "        'name': \"BGS2\",\n",
    "        'active_power': 'BGS2 Power',\n",
    "        'rpm': 'GEN SPEED BGS2',\n",
    "        'aux': 'BGS2-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [21.162, 8.49]\n",
    "    }],\n",
    "    'Karebbe': [{\n",
    "        'name': \"KGS1\",\n",
    "        'active_power': 'K U1 Active Power (MW)',\n",
    "        'rpm': 'K U1 Turb Gov Turbine Speed (RPM)',\n",
    "        'aux': 'KGS1-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [20.944, 11.398]\n",
    "    },\n",
    "    {\n",
    "        'name': \"KGS2\",\n",
    "        'active_power': 'K U2 Active Power (MW)',\n",
    "        'rpm': 'K U2 Turb Gov Turbine Speed (RPM)',\n",
    "        'aux': 'KGS2-Auxiliary Grid (0 = ACTIVE)',\n",
    "        'coef': [21.162, 8.49]\n",
    "    }]\n",
    "}\n",
    "\n",
    "for value in plant_metadata.values():\n",
    "    for value2 in value:\n",
    "        init_db_timeconst(['oee', 'phy_avail', 'performance', 'uo_Avail', \"aux_0\", \"aux_1\"], \"db/kpi.db\", value2['name'])\n",
    "\n",
    "init_db_timeconst(['hpd', 'ahpa', 'lpd', 'bpd', 'kpd'], \"db/kpi.db\", \"PowerProd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shutdown_and_snl_periods(df_selected, column_name):\n",
    "    data_timestamp = df_selected[['TimeStamp']].values\n",
    "    sensor_datas = df_selected[column_name].values\n",
    "\n",
    "    activepower_data = sensor_datas[:, 0].astype(float)\n",
    "    rpm_data = sensor_datas[:, 1].astype(float)\n",
    "\n",
    "    shutdown_mask = (activepower_data <= 3) & (rpm_data <= 10)\n",
    "    snl_mask = (activepower_data <= 3) & (rpm_data >= 259.35) & (rpm_data <= 286.65)\n",
    "\n",
    "    def extract_periods(mask):\n",
    "        change_points = np.diff(mask.astype(int), prepend=0)\n",
    "        start_indices = np.where(change_points == 1)[0]\n",
    "        end_indices = np.where(change_points == -1)[0]\n",
    "\n",
    "        if mask[-1]:\n",
    "            end_indices = np.append(end_indices, len(mask))\n",
    "        if mask[0]:\n",
    "            start_indices = np.insert(start_indices, 0, 0)\n",
    "\n",
    "        periods = []\n",
    "        for start, end in zip(start_indices, end_indices):\n",
    "            start_time = data_timestamp[start][0]\n",
    "            end_time = data_timestamp[end - 1][0]\n",
    "            periods.append((start_time, end_time))\n",
    "        return periods\n",
    "\n",
    "    shutdown_periods = extract_periods(shutdown_mask)\n",
    "    snl_periods = extract_periods(snl_mask)\n",
    "\n",
    "    return shutdown_periods, snl_periods\n",
    "\n",
    "def compute_oee_metrics(df_selected, column_name, shutdown_periods, snl_periods, performance_formula):\n",
    "    data_timestamp = df_selected[['TimeStamp']].values.flatten()\n",
    "    sensor_datas = df_selected[column_name].values\n",
    "\n",
    "    active_power = sensor_datas[:, 0].astype(float)\n",
    "\n",
    "    nonzeroneg_mask = active_power > 0\n",
    "    total_hours = (pd.to_datetime(str(data_timestamp[-1])) - pd.to_datetime(str(data_timestamp[0]))).total_seconds() / 3600\n",
    "\n",
    "    downtime_hours = sum(\n",
    "        (pd.to_datetime(str(end)) - pd.to_datetime(str(start))).total_seconds() / 3600\n",
    "        for start, end in shutdown_periods\n",
    "    )\n",
    "    snl_hours = sum(\n",
    "        (pd.to_datetime(str(end)) - pd.to_datetime(str(start))).total_seconds() / 3600\n",
    "        for start, end in snl_periods\n",
    "    )\n",
    "\n",
    "    phy_avail = max(round((total_hours - downtime_hours) / total_hours, 2), 0.01)\n",
    "    uo_Avail = max(round((total_hours - snl_hours) / total_hours, 2), 0.01)\n",
    "\n",
    "    if np.any(nonzeroneg_mask):\n",
    "        log_mean = np.mean(np.log(active_power[nonzeroneg_mask]))\n",
    "        performance = max(round((performance_formula[0] * log_mean + performance_formula[1]) / 100, 2), 0)\n",
    "    else:\n",
    "        performance = 0.01\n",
    "\n",
    "    oee = max(round(phy_avail * performance * uo_Avail, 2), 0.01)\n",
    "    datetime_nowMidnight = pd.to_datetime(str(data_timestamp[-1])).replace(hour=1, minute=0, second=0)\n",
    "\n",
    "    return datetime_nowMidnight, oee, phy_avail, performance, uo_Avail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.to_datetime('2024-01-01 01:00:00')\n",
    "end_time = master_pd['TimeStamp'].max()\n",
    "\n",
    "current_start = start_time\n",
    "while current_start < end_time:\n",
    "    current_end = current_start + pd.DateOffset(days=1)\n",
    "\n",
    "    mask = (master_pd['TimeStamp'] >= current_start) & (\n",
    "        master_pd['TimeStamp'] < current_end)\n",
    "    df_sel = master_pd.loc[mask]\n",
    "\n",
    "    for value in plant_metadata.values():\n",
    "        for tags in value:\n",
    "            unit_name = tags['name']\n",
    "\n",
    "            if tags['active_power'] not in df_sel.columns or tags['rpm'] not in df_sel.columns:\n",
    "                continue  # Skip if required data not present\n",
    "\n",
    "            df_unit = df_sel[['TimeStamp', tags['active_power'], tags['rpm'], tags['aux']]].dropna()\n",
    "            if df_unit.empty:\n",
    "                continue\n",
    "\n",
    "            # Process shutdown & SNL\n",
    "            shutdown_periods, snl_periods = process_shutdown_and_snl_periods(\n",
    "                df_unit, [tags['active_power'], tags['rpm']]\n",
    "            )\n",
    "\n",
    "            # Compute OEE and related KPIs\n",
    "            datetime_nowMidnight, oee, phy_avail, performance, uo_Avail = compute_oee_metrics(\n",
    "                df_unit, [tags['active_power'], tags['rpm']],\n",
    "                shutdown_periods, snl_periods,\n",
    "                performance_formula=tags['coef']\n",
    "            )\n",
    "\n",
    "            # Count Auxiliary Grid ON/OFF\n",
    "            counts_aux = df_unit[tags['aux']].value_counts().sort_index()\n",
    "            aux_0 = counts_aux.get(0.0, 0)\n",
    "            aux_1 = counts_aux.get(1.0, 0)\n",
    "\n",
    "            # Save to database\n",
    "            timeseries_savedb(\n",
    "                datetime_nowMidnight,\n",
    "                np.array([oee, phy_avail, performance, uo_Avail, aux_0, aux_1]),\n",
    "                ['oee', 'phy_avail', 'performance', 'uo_Avail', 'aux_0', 'aux_1'],\n",
    "                \"db/kpi.db\",\n",
    "                unit_name\n",
    "            )\n",
    "\n",
    "    pda_datas = df_sel[['Total Hydro Power Daily (Tot)', 'Avg Hydro Power Available 1D (Avg)' , 'Total Larona Power Daily (Tot)', 'Total Balambano Power Daily (Tot)', 'Total Karebbe Power Daily (Tot)']].mean().values\n",
    "    timeseries_savedb(\n",
    "            datetime_nowMidnight,\n",
    "            np.array([pda_datas[0], pda_datas[1], pda_datas[2], pda_datas[3], pda_datas[3]]).astype(np.float64),\n",
    "            ['hpd', 'ahpa', 'lpd', 'bpd', 'kpd'],\n",
    "            \"db/kpi.db\",\n",
    "            \"PowerProd\"\n",
    "        )\n",
    "    current_start = current_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_datas = commons.fetch_between_dates(\"2023-01-22T18:13:00\", \"2023-05-22T19:52:00\", \"db/original_data.db\", \"original_data\")\n",
    "data_timestamp = sensor_datas[:, 1]\n",
    "sensor_datas = sensor_datas[:, 2:].astype(float)\n",
    "\n",
    "frame_len = 286\n",
    "num_frames = len(sensor_datas) // frame_len  # 120\n",
    "usable_len = frame_len * num_frames  # 34320\n",
    "\n",
    "data_timestamp = data_timestamp[:usable_len]\n",
    "sensor_datas = sensor_datas[:usable_len]\n",
    "\n",
    "frames_timestamp = data_timestamp.reshape(num_frames, frame_len)\n",
    "frames_sensor = sensor_datas.reshape(num_frames, frame_len, 30)\n",
    "for i_frame, frame in enumerate(frames_sensor):\n",
    "    now_timestamp = frames_timestamp[i_frame, :]\n",
    "    now_sensors = frames_sensor[i_frame, :]\n",
    "\n",
    "    shutdown_periods = commons.process_shutdownTimestamp(now_timestamp, now_sensors)\n",
    "    snl_periods = commons.process_SNLTimestamp(now_timestamp, now_sensors)\n",
    "\n",
    "    nonzeroneg_activepower = now_sensors[:, 0] > 0\n",
    "    total_hours = (datetime.fromisoformat(str(now_timestamp[-1])) - datetime.fromisoformat(str(now_timestamp[0]))).total_seconds() / 3600\n",
    "    downtime_hours = 0\n",
    "    for datespan_downtime in shutdown_periods:\n",
    "        delta = datetime.fromisoformat(str(datespan_downtime[1])) - datetime.fromisoformat(str(datespan_downtime[0]))\n",
    "        delta_hours = delta.total_seconds() / 3600\n",
    "        downtime_hours += delta_hours\n",
    "\n",
    "    snl_hours = 0\n",
    "    for datespan_snl in snl_periods:\n",
    "        delta = datetime.fromisoformat(str(datespan_snl[1])) - datetime.fromisoformat(str(datespan_snl[0]))\n",
    "        delta_hours = delta.total_seconds() / 3600\n",
    "        snl_hours += delta_hours\n",
    "\n",
    "    phy_avail = max(round(((total_hours - downtime_hours) / total_hours), 2), 0.01)\n",
    "    uo_Avail = max(round(((total_hours - snl_hours) / total_hours), 2), 0.01)\n",
    "    if len(now_sensors[nonzeroneg_activepower, 0]) > 0:\n",
    "        performance = max(round((20.944 * np.mean(np.log(now_sensors[nonzeroneg_activepower, 0])) + 11.398)/100, 2), 0)\n",
    "    else:\n",
    "        performance = 0.01\n",
    "    \n",
    "    oee = max(round(phy_avail * performance * uo_Avail,2), 0.01)\n",
    "    datetime_nowMidnight = datetime.fromisoformat(str(now_timestamp[-1])).replace(hour=1, minute=0, second=0)\n",
    "\n",
    "    timeseries_savedb(datetime_nowMidnight, np.array([oee, phy_avail, performance, uo_Avail]), ['oee', 'phy_avail', 'performance', 'uo_Avail'], \"db/kpi.db\", \"kpi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_datas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "286 / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "from django.conf import settings\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "feature_set = ['Active Power', 'Reactive Power', 'Governor speed actual', 'UGB X displacement', 'UGB Y displacement',\n",
    "    'LGB X displacement', 'LGB Y displacement', 'TGB X displacement',\n",
    "    'TGB Y displacement', 'Stator winding temperature 13',\n",
    "    'Stator winding temperature 14', 'Stator winding temperature 15',\n",
    "    'Surface Air Cooler Air Outlet Temperature',\n",
    "    'Surface Air Cooler Water Inlet Temperature',\n",
    "    'Surface Air Cooler Water Outlet Temperature',\n",
    "    'Stator core temperature', 'UGB metal temperature',\n",
    "    'LGB metal temperature 1', 'LGB metal temperature 2',\n",
    "    'LGB oil temperature', 'Penstock Flow', 'Turbine flow',\n",
    "    'UGB cooling water flow', 'LGB cooling water flow',\n",
    "    'Generator cooling water flow', 'Governor Penstock Pressure',\n",
    "    'Penstock pressure', 'Opening Wicked Gate', 'UGB Oil Contaminant',\n",
    "    'Gen Thrust Bearing Oil Contaminant']\n",
    "\n",
    "model_array = [\"Attention\", \"DTAAD\", \"MAD_GAN\", \"TranAD\", \"DAGMM\", \"USAD\", \"OmniAnomaly\"]\n",
    "with open('model_thr.pickle', 'rb') as handle:\n",
    "    model_thr = pickle.load(handle)\n",
    "\n",
    "with open('normalize_2023.pickle', 'rb') as handle:\n",
    "    normalize_obj = pickle.load(handle)\n",
    "    min_a, max_a = normalize_obj['min_a'], normalize_obj['max_a']\n",
    "\n",
    "def normalize3(a, min_a=None, max_a=None):\n",
    "    if min_a is None: min_a, max_a = np.min(a, axis=0), np.max(a, axis=0)\n",
    "    return ((a - min_a) / (max_a - min_a + 0.0001)), min_a, max_a\n",
    "\n",
    "def denormalize3(a_norm, min_a, max_a):\n",
    "    return a_norm * (max_a - min_a + 0.0001) + min_a\n",
    "\n",
    "\n",
    "def fetch_between_dates(start_date, end_date, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT * FROM {table_name} WHERE timestamp BETWEEN ? AND ?\n",
    "    \"\"\", (start_date, end_date))\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not rows:\n",
    "        return np.array([])\n",
    "    \n",
    "    return np.array(rows)\n",
    "\n",
    "def fetch_last_rows(num_row, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT * FROM {table_name} ORDER BY timestamp DESC LIMIT ?\n",
    "    \"\"\", (num_row,))\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not rows:\n",
    "        return np.array([])\n",
    "    \n",
    "    return np.array(rows)\n",
    "\n",
    "def convert_timestamp(timestamp_str):\n",
    "    dt = datetime.fromisoformat(timestamp_str)\n",
    "    return pd.Timestamp(dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "def percentage2severity(value):\n",
    "    return (\n",
    "        1 if 0 <= value < 5 else\n",
    "        2 if 5 <= value < 20 else\n",
    "        3 if 20 <= value < 40 else\n",
    "        4 if 40 <= value < 75 else\n",
    "        5 if 75 <= value <= 100 else\n",
    "        6\n",
    "    )\n",
    "    \n",
    "def calc_counterPercentage(threshold_percentages):\n",
    "    counter_feature = {}\n",
    "    for modex_idx, values_pred in threshold_percentages.items():\n",
    "        values_pred = dict(sorted(values_pred.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "        for name_feat, percentage in values_pred.items():\n",
    "            if name_feat in counter_feature:\n",
    "                counter_feature[name_feat][\"count\"] = counter_feature[name_feat][\"count\"] + 1\n",
    "                counter_feature[name_feat][\"percentage\"] = counter_feature[name_feat][\"percentage\"] + percentage\n",
    "            else:\n",
    "                counter_feature[name_feat] = {\"count\": 1, \"percentage\": percentage}\n",
    "\n",
    "    counter_feature_s1 = dict(sorted(counter_feature.items(), key=lambda item: item[1]['count'], reverse=True)[:10])\n",
    "    counter_feature_s2 = dict(sorted(counter_feature_s1.items(), key=lambda item: item[1]['percentage'] // len(model_array), reverse=True))\n",
    "    #counter_feature_s2_rank = dict(sorted(counter_feature_s1.items(), key=lambda item: item[1]['count'], reverse=True))\n",
    "\n",
    "    for key, value in counter_feature_s2.items():\n",
    "        counter_feature_s2[key]['count'] = (counter_feature_s2[key]['count'] / len(model_array)) * 100\n",
    "        counter_feature_s2[key]['severity'] = percentage2severity(counter_feature_s2[key]['percentage'] // len(model_array))\n",
    "        counter_feature_s2[key]['percentage'] = (counter_feature_s2[key]['percentage'] // len(model_array))\n",
    "\n",
    "    # Find Which Model Have Highest Confidence\n",
    "    counter_feature_plot = {}\n",
    "    for index, value in counter_feature_s2.items():\n",
    "        higher_data = {\"model\": 0, \"percentage\": 0}\n",
    "        for model_idx in threshold_percentages:\n",
    "            if index in threshold_percentages[model_idx]:\n",
    "                if higher_data[\"percentage\"] <= threshold_percentages[model_idx][index]:\n",
    "                    higher_data[\"model\"] = model_idx\n",
    "                    higher_data[\"percentage\"] = threshold_percentages[model_idx][index]\n",
    "        \n",
    "        counter_feature_plot[index] = higher_data['model']\n",
    "\n",
    "    return counter_feature_s2, counter_feature_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set.index(\"LGB cooling water flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import medfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hampel_filter(series, window_size=3, n_sigmas=3):\n",
    "    new_series = series.copy()\n",
    "    k = 1.4826  # scale factor for Gaussian distribution\n",
    "    n = len(series)\n",
    "\n",
    "    for i in range(window_size, n - window_size):\n",
    "        window = series[i - window_size:i + window_size + 1]\n",
    "        median = np.median(window)\n",
    "        mad = k * np.median(np.abs(window - median))\n",
    "        if np.abs(series[i] - median) > n_sigmas * mad:\n",
    "            new_series[i] = median\n",
    "    return new_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = \"2021-05-28T05:55:00\"\n",
    "start_date = \"2021-04-28T06:10:00\"\n",
    "\n",
    "severity_trending_datas = fetch_between_dates(start_date, end_date, \"db/severity_trendings.db\", \"severity_trendings\")\n",
    "sensor_datas = fetch_between_dates(start_date, end_date, \"db/severity_trendings.db\", \"original_sensor\")\n",
    "    \n",
    "data_timestamp = sensor_datas[:, 1]\n",
    "severity_trending_datas = severity_trending_datas[:, 2:].astype(float)\n",
    "sensor_datas = sensor_datas[:, 2:].astype(float)\n",
    "\n",
    "for i in range(len(feature_set)):\n",
    "    severity_trending_datas[:, i] = hampel_filter(severity_trending_datas[:, i], window_size=300, n_sigmas=10)\n",
    "\n",
    "datetime_index = pd.to_datetime(data_timestamp)\n",
    "series = pd.Series(severity_trending_datas[:, 23], index=datetime_index)\n",
    "series = series.asfreq('15min')\n",
    "\n",
    "result = seasonal_decompose(series, model='additive', period=96 * 2)\n",
    "result.plot()\n",
    "\n",
    "trend = result.trend.dropna()\n",
    "\n",
    "x = np.arange(len(trend))\n",
    "corr, _ = spearmanr(x, trend)\n",
    "print(\"Spearman correlation:\", corr)\n",
    "\n",
    "if corr > 0.3:\n",
    "    priority = 1  # increasing\n",
    "elif corr < -0.3:\n",
    "    priority = 3  # decreasing\n",
    "else:\n",
    "    priority = 2  # flat\n",
    "\n",
    "priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def extract_time_series_features(x):\n",
    "    x = np.asarray(x)\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Basic statistics\n",
    "    features['mean'] = np.mean(x)\n",
    "    features['min'] = np.min(x)\n",
    "    features['max'] = np.max(x)\n",
    "    features['std'] = np.std(x)\n",
    "    features['range'] = np.ptp(x)  # max - min\n",
    "    features['mad'] = np.mean(np.abs(x - np.mean(x)))  # Mean Absolute Deviation\n",
    "    features['rms'] = np.sqrt(np.mean(x**2))  # Root Mean Square\n",
    "    features['skewness'] = skew(x)\n",
    "    features['kurtosis'] = kurtosis(x)\n",
    "\n",
    "    # Monotonicity (fraction of increasing / decreasing)\n",
    "    diffs = np.diff(x)\n",
    "    features['monotonic_increasing'] = np.sum(diffs > 0) / len(diffs)\n",
    "    features['monotonic_decreasing'] = np.sum(diffs < 0) / len(diffs)\n",
    "\n",
    "    # Slope (linear trend) and gradient\n",
    "    if len(x) > 1:\n",
    "        t = np.arange(len(x))\n",
    "        slope = np.polyfit(t, x, 1)[0]\n",
    "        features['slope'] = slope\n",
    "    else:\n",
    "        features['slope'] = np.nan\n",
    "\n",
    "    features['gradient_mean'] = np.mean(np.gradient(x))\n",
    "\n",
    "    df = pd.DataFrame([features])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap = 0.2\n",
    "window_size = 1492\n",
    "step_size = int(window_size * (1 - overlap))\n",
    "\n",
    "for start in range(0, len(severity_trending_datas) - window_size + 1, step_size):\n",
    "    #windows.append(data[start:start + window_size])\n",
    "    print(start, start + window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.gradient(severity_trending_datas[:, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(severity_trending_datas[:, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "data_pca = pca.fit_transform(severity_trending_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_pca[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_trending.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "current_trending = severity_trending_datas[:, 8] # 8 22\n",
    "current_trending = np.convolve(current_trending, kernel, mode='same')\n",
    "\n",
    "y = (current_trending - 0) / (100 - 0)\n",
    "x = np.linspace(0, 30, len(y)).reshape(-1, 1) \n",
    "\n",
    "reg1 = LinearRegression()\n",
    "reg1.fit(x, y)\n",
    "\n",
    "y_linear1 = reg1.predict(x)\n",
    "\n",
    "plt.plot(x, y, label=\"Unknown Function\", color='blue')\n",
    "plt.plot(x, y_linear1, label=\"Linear Regression Fit\", linestyle=\"dashed\", color='orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "reg1.coef_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = LinearRegression()\n",
    "reg2 = LinearRegression()\n",
    "\n",
    "x = np.linspace(0, 10, 100).reshape(-1, 1) \n",
    "y = (1 / (0.001 + np.exp(-(x - 5))))\n",
    "y_rev = 1- (1 / (1 + np.exp(-(x - 5))))\n",
    "\n",
    "reg1.fit(x, y)\n",
    "reg2.fit(x, y_rev)\n",
    "\n",
    "y_linear1 = reg1.predict(x)\n",
    "y_linear2 = reg2.predict(x)\n",
    "\n",
    "m1 = reg1.coef_[0]\n",
    "m2 = reg2.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, label=\"Unknown Function\", color='blue')\n",
    "plt.plot(x, y_linear1, label=\"Linear Regression Fit\", linestyle=\"dashed\", color='orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, y_rev, label=\"Unknown Function\", color='blue')\n",
    "plt.plot(x, y_linear2, label=\"Linear Regression Fit\", linestyle=\"dashed\", color='orange')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = LinearRegression()\n",
    "reg2 = LinearRegression()\n",
    "\n",
    "x = np.linspace(0, 10, 100).reshape(-1, 1) \n",
    "y = 10 * x\n",
    "y_rev = -10 * x\n",
    "\n",
    "reg1.fit(x, y)\n",
    "reg2.fit(x, y_rev)\n",
    "\n",
    "y_linear1 = reg1.predict(x)\n",
    "y_linear2 = reg2.predict(x)\n",
    "\n",
    "m1 = reg1.coef_[0]\n",
    "m2 = reg2.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg.fit(x, y)  # Fit y = mx + b\n",
    "m = reg.coef_[0]  # Extract slope (gradient)\n",
    "b = reg.intercept_  # Extract intercept\n",
    "\n",
    "# Compute predicted values from regression\n",
    "y_linear = reg.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 100)\n",
    "positive_gradients = [0.5 * x, x, 10 * x]  # Increasing slopes\n",
    "stationary_gradient = np.zeros_like(x)  # Flat line\n",
    "negative_gradients = [-0.5 * x, -x, -2 * x]  # Decreasing slopes\n",
    "\n",
    "# Define labels\n",
    "labels = ['Positive (0.5x)', 'Positive (x)', 'Positive (2x)',\n",
    "          'Stationary (0)',\n",
    "          'Negative (-0.5x)', 'Negative (-x)', 'Negative (-2x)']\n",
    "\n",
    "# Define colors\n",
    "colors = ['green', 'lime', 'darkgreen', 'black', 'red', 'orange', 'darkred']\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot positive gradients\n",
    "for i, y in enumerate(positive_gradients):\n",
    "    plt.plot(x, y, label=labels[i], color=colors[i])\n",
    "\n",
    "# Plot stationary gradient\n",
    "plt.plot(x, stationary_gradient, label=labels[3], color=colors[3], linestyle='dashed')\n",
    "\n",
    "# Plot negative gradients\n",
    "for i, y in enumerate(negative_gradients, start=4):\n",
    "    plt.plot(x, y, label=labels[i], color=colors[i])\n",
    "\n",
    "# Customize plot\n",
    "plt.axhline(0, color='gray', linewidth=0.5)\n",
    "plt.axvline(0, color='gray', linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Plot of Positive, Stationary, and Negative Gradients\")\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Y-axis\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.dates import DateFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage2severity(value):\n",
    "    return (\n",
    "        1 if 0 <= value < 5 else\n",
    "        2 if 5 <= value < 20 else\n",
    "        3 if 20 <= value < 40 else\n",
    "        4 if 40 <= value < 75 else\n",
    "        5 if 75 <= value <= 100 else\n",
    "        6\n",
    "    )\n",
    "    \n",
    "def calc_counterPercentage(threshold_percentages):\n",
    "    counter_feature = {}\n",
    "    for modex_idx, values_pred in threshold_percentages.items():\n",
    "        values_pred = dict(sorted(values_pred.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "        for name_feat, percentage in values_pred.items():\n",
    "            if name_feat in counter_feature:\n",
    "                counter_feature[name_feat][\"count\"] = counter_feature[name_feat][\"count\"] + 1\n",
    "                counter_feature[name_feat][\"percentage\"] = counter_feature[name_feat][\"percentage\"] + percentage\n",
    "            else:\n",
    "                counter_feature[name_feat] = {\"count\": 1, \"percentage\": percentage}\n",
    "\n",
    "    counter_feature_s1 = dict(sorted(counter_feature.items(), key=lambda item: item[1]['count'], reverse=True)[:10])\n",
    "    counter_feature_s2 = dict(sorted(counter_feature_s1.items(), key=lambda item: item[1]['percentage'] // len(model_array), reverse=True))\n",
    "    #counter_feature_s2_rank = dict(sorted(counter_feature_s1.items(), key=lambda item: item[1]['count'], reverse=True))\n",
    "\n",
    "    for key, value in counter_feature_s2.items():\n",
    "        counter_feature_s2[key]['count'] = (counter_feature_s2[key]['count'] / len(model_array)) * 100\n",
    "        counter_feature_s2[key]['severity'] = percentage2severity(counter_feature_s2[key]['percentage'] // len(model_array))\n",
    "        counter_feature_s2[key]['percentage'] = (counter_feature_s2[key]['percentage'] // len(model_array))\n",
    "\n",
    "    # Find Which Model Have Highest Confidence\n",
    "    counter_feature_plot = {}\n",
    "    for index, value in counter_feature_s2.items():\n",
    "        higher_data = {\"model\": 0, \"percentage\": 0}\n",
    "        for model_idx in threshold_percentages:\n",
    "            if index in threshold_percentages[model_idx]:\n",
    "                if higher_data[\"percentage\"] <= threshold_percentages[model_idx][index]:\n",
    "                    higher_data[\"model\"] = model_idx\n",
    "                    higher_data[\"percentage\"] = threshold_percentages[model_idx][index]\n",
    "        \n",
    "        counter_feature_plot[index] = higher_data['model']\n",
    "\n",
    "    return counter_feature_s2, counter_feature_plot\n",
    "\n",
    "def calc_counterPercentageTrending(threshold_percentages):\n",
    "    counter_feature = {}\n",
    "    for modex_idx, values_pred in threshold_percentages.items():\n",
    "        for name_feat, percentage in values_pred.items():\n",
    "            if name_feat in counter_feature:\n",
    "                if percentage > 5.0:\n",
    "                    counter_feature[name_feat][\"count\"] = counter_feature[name_feat][\"count\"] + 1\n",
    "                    counter_feature[name_feat][\"percentage\"] = counter_feature[name_feat][\"percentage\"] + percentage\n",
    "            else:\n",
    "                counter_feature[name_feat] = {\"count\": 1, \"percentage\": percentage}\n",
    "\n",
    "    for key, value in counter_feature.items():\n",
    "        counter_feature[key]['count'] = (counter_feature[key]['count'] / len(model_array)) * 100\n",
    "        if counter_feature[key]['count'] >= 20.0:\n",
    "            counter_feature[key]['severity'] = percentage2severity(counter_feature[key]['percentage'] // len(model_array))\n",
    "            counter_feature[key]['percentage'] = (counter_feature[key]['percentage'] // len(model_array))\n",
    "        else:\n",
    "            counter_feature[key]['severity'] = 1\n",
    "            counter_feature[key]['percentage'] = 0.0\n",
    "\n",
    "    return counter_feature\n",
    "\n",
    "def do_plotSeverityRank():\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    gs = GridSpec(4, 3, figure=fig)\n",
    "\n",
    "    feature_index_list = [feature_set.index(feat_name) for feat_name in list(counter_feature_s2.keys())]\n",
    "    for idx, (feature_index_now) in enumerate(feature_index_list[:4]):\n",
    "        model_idx_highest = counter_feature_plot[feature_set[feature_index_now]]\n",
    "\n",
    "        ax = fig.add_subplot(gs[idx, :2])\n",
    "        ax.plot(df_timestamp, temp_ypreds[model_idx_highest][:, feature_index_now], color='blue', label='Prediction')\n",
    "        ax.plot(df_timestamp, df_feature[:, feature_index_now], color='red', label='Original')\n",
    "        ax.set_title(feature_set[feature_index_now])\n",
    "        ax.legend() \n",
    "        ax.grid(True)\n",
    "\n",
    "    date_format = DateFormatter(\"%d/%m/%Y - %H:%M\")  # Define the desired format\n",
    "    plt.gca().xaxis.set_major_formatter(date_format)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    y2 = list(counter_feature_s2.keys())\n",
    "    x2 = [value['severity'] for value in counter_feature_s2.values()]\n",
    "    x2_c = [value['count'] for value in counter_feature_s2.values()]\n",
    "\n",
    "    norm_x2 = [(val - 1) / 5 for val in x2]\n",
    "    cmap = LinearSegmentedColormap.from_list('severity_colormap', ['green', 'yellow', 'red'])\n",
    "    colors = [cmap(norm) for norm in norm_x2]\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[:3, 2])\n",
    "    bars = ax3.barh(y2, x2, color=colors)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=1, vmax=6))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax3, orientation='vertical', label='Severity')\n",
    "\n",
    "    for bar, perc in zip(bars, x2_c):\n",
    "        width = bar.get_width()  # Get the width of the bar\n",
    "        ax3.text(\n",
    "            width - 0.1,             # X-coordinate (inside the bar, near the right edge)\n",
    "            bar.get_y() + bar.get_height() / 2,  # Y-coordinate (center of the bar)\n",
    "            f\"{int(perc)}%\",            # Text label (percentage with % sign)\n",
    "            va='center',           # Vertical alignment\n",
    "            ha='right',            # Horizontal alignment\n",
    "            color='black',         # Text color for visibility\n",
    "            fontsize=9            # Font size\n",
    "        )\n",
    "    ax3.invert_yaxis()\n",
    "    ax3.set_xticks(range(1, 8))\n",
    "    ax3.set_ylabel(\"Parameter\")\n",
    "    ax3.set_xlabel(\"Severity\")\n",
    "    ax3.set_title(\"Severity Rank\")\n",
    "\n",
    "    #fig.suptitle(f\"{df_anomaly_unplaned.values[failure_index_list, 4]}_{df_anomaly_unplaned.values[failure_index_list, 0]}\", fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def fetch_between_dates(start_date, end_date, db_name=\"data.db\", table_name=\"sensor_data\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT * FROM {table_name} WHERE timestamp BETWEEN ? AND ?\n",
    "    \"\"\", (start_date, end_date))\n",
    "    \n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "\n",
    "    if not rows:\n",
    "        return np.array([])\n",
    "    \n",
    "    return np.array(rows)\n",
    "\n",
    "def convert_timestamp(timestamp_str):\n",
    "    dt = datetime.fromisoformat(timestamp_str)\n",
    "    return pd.Timestamp(dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = ['Active Power', 'Reactive Power', 'Governor speed actual', 'UGB X displacement', 'UGB Y displacement',\n",
    "    'LGB X displacement', 'LGB Y displacement', 'TGB X displacement',\n",
    "    'TGB Y displacement', 'Stator winding temperature 13',\n",
    "    'Stator winding temperature 14', 'Stator winding temperature 15',\n",
    "    'Surface Air Cooler Air Outlet Temperature',\n",
    "    'Surface Air Cooler Water Inlet Temperature',\n",
    "    'Surface Air Cooler Water Outlet Temperature',\n",
    "    'Stator core temperature', 'UGB metal temperature',\n",
    "    'LGB metal temperature 1', 'LGB metal temperature 2',\n",
    "    'LGB oil temperature', 'Penstock Flow', 'Turbine flow',\n",
    "    'UGB cooling water flow', 'LGB cooling water flow',\n",
    "    'Generator cooling water flow', 'Governor Penstock Pressure',\n",
    "    'Penstock pressure', 'Opening Wicked Gate', 'UGB Oil Contaminant',\n",
    "    'Gen Thrust Bearing Oil Contaminant']\n",
    "\n",
    "# feature_set = ['Active Power', 'Governor speed actual', \n",
    "#     'UGB X displacement', 'UGB Y displacement', 'LGB X displacement', 'LGB Y displacement', 'TGB X displacement', 'TGB Y displacement', \n",
    "#     'Stator core temperature', 'Stator winding temperature 13', 'Stator winding temperature 14', 'Stator winding temperature 15',\n",
    "#     'Surface Air Cooler Air Outlet Temperature', 'Surface Air Cooler Water Inlet Temperature', 'Surface Air Cooler Water Outlet Temperature',\n",
    "#     'Gen Voltage Phase 1', 'Gen Voltage Phase 2', 'Gen Voltage Phase 3',\n",
    "#     'Gen Current Phase 1', 'Gen Current Phase 2', 'Gen Current Phase 3', \n",
    "#     'UGB metal temperature', 'LGB metal temperature 1', 'LGB metal temperature 2',\n",
    "#     'UGB oil temperature', 'LGB oil temperature', 'UGB cooling water flow', 'LGB cooling water flow', 'Generator cooling water flow',\n",
    "#     'UGB Oil Contaminant', 'Gen Thrust Bearing Oil Contaminant',\n",
    "#     'Penstock Flow', 'Turbine flow', 'Governor Penstock Pressure', 'Penstock pressure', 'Opening Wicked Gate']\n",
    "\n",
    "model_array = [\"Attention\", \"DTAAD\", \"MAD_GAN\", \"TranAD\", \"DAGMM\", \"USAD\", \"OmniAnomaly\"]\n",
    "\n",
    "# window_size = 15\n",
    "# kernel = np.ones(window_size) / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect(\"db_data/original_data.db\")\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute(f\"\"\"SELECT * FROM original_data order by rowid desc LIMIT 1\"\"\")\n",
    "# rows = cursor.fetchall()\n",
    "# conn.close()\n",
    "# last_date = np.datetime64(np.array(rows)[:, 1][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates_lastest = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%S\") #\"2025-03-27T05:36:00\" \n",
    "timestamp = datetime.strptime(end_dates_lastest, \"%Y-%m-%dT%H:%M:%S\")\n",
    "hours_2before = timestamp - timedelta(hours=2)\n",
    "beofre_15min = timestamp - timedelta(minutes=60)\n",
    "hours_2before_str = hours_2before.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "threshold_percentages = {}\n",
    "for idx_model, (model_name) in enumerate(model_array):\n",
    "    now_fetched = fetch_between_dates(beofre_15min.strftime(\"%Y-%m-%dT%H:%M:%S\"), end_dates_lastest, \"db/threshold_data.db\", model_name)[-1, 2:]\n",
    "\n",
    "    threshold_pass = {}\n",
    "    for idx_sensor, sensor_thre in enumerate(now_fetched):\n",
    "        threshold_pass[feature_set[idx_sensor]] = float(sensor_thre)\n",
    "\n",
    "    threshold_percentages[idx_model] = threshold_pass\n",
    "\n",
    "temp_original_data = fetch_between_dates(hours_2before_str, end_dates_lastest, \"db/original_data.db\", \"original_data\")\n",
    "df_timestamp, df_feature = temp_original_data[:, 1], temp_original_data[:, 2:].astype(np.float16)\n",
    "df_timestamp = np.array([convert_timestamp(now_str) for now_str in df_timestamp])\n",
    "\n",
    "temp_ypreds = {}\n",
    "for idx_model, (model_name) in enumerate(model_array):\n",
    "    temp_ypreds[idx_model] = fetch_between_dates(hours_2before_str, end_dates_lastest, \"db/pred_data.db\", model_name)[:, 2:].astype(np.float16)\n",
    "\n",
    "counter_feature_s2, counter_feature_plot = calc_counterPercentage(threshold_percentages)\n",
    "df_feature_send = []\n",
    "y_pred_send = []\n",
    "\n",
    "feature_index_list = [feature_set.index(feat_name) for feat_name in list(counter_feature_s2.keys())]\n",
    "for idx, (feature_index_now) in enumerate(feature_index_list[:4]):\n",
    "    model_idx_highest = counter_feature_plot[feature_set[feature_index_now]]\n",
    "\n",
    "    df_feature_send.append(temp_ypreds[model_idx_highest][:, feature_index_now])\n",
    "    y_pred_send.append(df_feature[:, feature_index_now])\n",
    "\n",
    "df_feature_send = np.vstack(df_feature_send).T\n",
    "y_pred_send = np.vstack(y_pred_send).T\n",
    "\n",
    "# To Send counter_feature_s2, df_feature_send, y_pred_send\n",
    "\n",
    "fig = do_plotSeverityRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "l = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_send = []\n",
    "y_pred_send = []\n",
    "loss_send = []\n",
    "thr_now_model = []\n",
    "\n",
    "feature_index_list = [feature_set.index(feat_name) for feat_name in list(counter_feature_s2.keys())]\n",
    "for idx, (feature_index_now) in enumerate(feature_index_list[:4]):\n",
    "    model_idx_highest = counter_feature_plot[feature_set[feature_index_now]]\n",
    "\n",
    "    df_feature_send.append(temp_ypreds[model_idx_highest][:, feature_index_now])\n",
    "    y_pred_send.append(df_feature[:, feature_index_now])\n",
    "    \n",
    "    loss_send.append(df_feature[:, feature_index_now])\n",
    "    thr_now_model.append(float(model_thr[model_array[model_idx_highest]][feature_index_now]))\n",
    "\n",
    "\n",
    "# df_feature_send = np.vstack(df_feature_send).T\n",
    "# y_pred_send = np.vstack(y_pred_send).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ypreds[model_idx_highest][:, feature_index_now]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l(, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature_send[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr_now_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_feature = {}\n",
    "for modex_idx, values_pred in threshold_percentages.items():\n",
    "    values_pred = dict(sorted(values_pred.items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "    for name_feat, percentage in values_pred.items():\n",
    "        if name_feat in counter_feature:\n",
    "            counter_feature[name_feat][\"count\"] = counter_feature[name_feat][\"count\"] + 1\n",
    "            counter_feature[name_feat][\"percentage\"] = counter_feature[name_feat][\"percentage\"] + percentage\n",
    "        else:\n",
    "            counter_feature[name_feat] = {\"count\": 1, \"percentage\": percentage}\n",
    "\n",
    "counter_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_feature_s1 = dict(sorted(counter_feature.items(), key=lambda item: item[1]['count'], reverse=True)[:10])\n",
    "counter_feature_s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_dates_lastest = \"2025-03-27T05:36:00\" \n",
    "timestamp = datetime.strptime(end_dates_lastest, \"%Y-%m-%dT%H:%M:%S\")\n",
    "hours_2before = timestamp - timedelta(days=30)\n",
    "hours_2before_str = hours_2before.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "data_between_dates = fetch_between_dates(hours_2before_str, end_dates_lastest, \"db/severity_trendings.db\", \"severity_trendings\")\n",
    "\n",
    "data_feature = data_between_dates[:, 2:].astype(float)\n",
    "data_timestamp = np.array([convert_timestamp(now_str) for now_str in data_between_dates[:, 1]])\n",
    "\n",
    "fig, axes = plt.subplots(30, 1, figsize=(10, 75))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(data_timestamp, scipy.signal.savgol_filter(data_feature[:, i], 50, 3))  # Plot data for each row\n",
    "    ax.set_title(f'{feature_set[i]} ', fontsize=10)  # Set title\n",
    "    #ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_ylabel(\"Severity Percentage\")\n",
    "    #ax.set_xticks(data_timestamp[::7])  # Reduce number of ticks\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denormalize3(a_norm, min_a, max_a):\n",
    "#     return a_norm * (max_a - min_a + 0.0001) + min_a\n",
    "\n",
    "# with open('normalize_2023.pickle', 'rb') as handle:\n",
    "#     normalize_obj = pickle.load(handle)\n",
    "#     min_a, max_a = normalize_obj['min_a'], normalize_obj['max_a']\n",
    "\n",
    "# with open('model_thr.pickle', 'rb') as handle:\n",
    "#     model_thr = pickle.load(handle)\n",
    "\n",
    "# for model_now in model_array:\n",
    "#     model_thr[model_now] = denormalize3(np.array(model_thr[model_now]), min_a, max_a).tolist()\n",
    "\n",
    "# with open('model_thr.pickle', 'wb') as handle:\n",
    "#     pickle.dump(model_thr, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
